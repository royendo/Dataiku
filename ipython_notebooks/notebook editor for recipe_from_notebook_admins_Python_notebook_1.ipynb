{
  "metadata": {
    "kernelspec": {
      "display_name": "Python (env Python37)",
      "name": "py-dku-venv-python37",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "version": "3.7.9",
      "name": "python",
      "pygments_lexer": "ipython3",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "tags": [
      "recipe-editor"
    ],
    "associatedRecipe": "recipe_from_notebook_admins_Python_notebook_1",
    "createdOn": 1626324275903,
    "hide_input": false,
    "customFields": {},
    "creator": "admin",
    "modifiedBy": "admin"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 1,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nfrom dataiku import spark as dkuspark\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.stat import Correlation\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.sql.types import StructType, StructField, StringType\n\nimport math\nimport pandas as pd\nimport numpy as np\n\nsc \u003d SparkContext.getOrCreate()\nsqlContext \u003d SQLContext(sc)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/Users/rendo/Library/DataScienceStudio/dss_home/code-envs/python/Python37/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n  warnings.warn(msg)\n"
        },
        {
          "output_type": "error",
          "evalue": "No module named \u0027pyspark\u0027",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-1-2382e54d198f\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# -*- coding: utf-8 -*-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataiku\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataiku\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdkuspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/dataiku/install/dataiku-dss-9.0.3-osx/python/dataiku/spark/__init__.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# saveDataFrame(df2, ds2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named \u0027pyspark\u0027"
          ],
          "ename": "ModuleNotFoundError"
        }
      ]
    },
    {
      "execution_count": 3,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Read recipe inputs\nfitness \u003d dataiku.Dataset(\"FITNESS\")\nfitness_df \u003d dkuspark.get_dataframe(sqlContext, fitness)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "evalue": "name \u0027dataiku\u0027 is not defined",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-3-5d39e78e5842\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read recipe inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0mfitness\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mdataiku\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FITNESS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfitness_df\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mdkuspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitness\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name \u0027dataiku\u0027 is not defined"
          ],
          "ename": "NameError"
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fitness_df \u003d fitness_df.dropna()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "column_names \u003d fitness_df.columns\ncorr_method \u003d dataiku.get_custom_variables()[\"corr_method\"]\nvector_col \u003d \"corr_features\""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "corr_method"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# convert to vector column first\nassembler \u003d VectorAssembler(inputCols\u003dfitness_df.columns, outputCol\u003dvector_col)\ndf_vector \u003d assembler.transform(fitness_df).select(vector_col)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "matrix \u003d Correlation.corr(df_vector, vector_col, method\u003dcorr_method)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "type(matrix)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "matrix.show(truncate\u003dFalse)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cor_np \u003d matrix.collect()[0][\"pearson({})\".format(vector_col)].values\ncor_np"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "type(cor_np)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dim \u003d int(math.sqrt(len(cor_np)))\ncor_mat \u003d cor_np.reshape((dim,dim))"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cor_mat"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Approach 1. - Failed\ndf_spark \u003d sqlContext.createDataFrame(pd.DataFrame(cor_mat))\ndf_spark.show()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Approach 2. - Failed\ncorr_data\u003d pd.DataFrame(cor_mat, index\u003d column_names,columns \u003d column_names).reset_index()\ncorr_pyspark_df \u003d spark.createDataFrame(corr_data)\ncorr_pyspark_df.show()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write recipe outputs\ncorr_pyspark \u003d dataiku.Dataset(\"corr_pyspark\")\ndkuspark.write_with_schema(corr_pyspark, df_spark)"
      ],
      "outputs": []
    }
  ]
}