{
  "metadata": {
    "kernelspec": {
      "display_name": "Python in GPU (env Obj_Detection)",
      "name": "py-dku-containerized-venv-obj_detection-gpu",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "version": "3.6.8",
      "name": "python",
      "pygments_lexer": "ipython3",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "tags": [
      "recipe-editor"
    ],
    "associatedRecipe": "compute_UrfVTR9K",
    "createdOn": 1639504733371,
    "hide_input": false,
    "customFields": {},
    "creator": "cameron.arnold@saic.com",
    "modifiedBy": "cameron.arnold@saic.com"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 1,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\n\nimport torch, torchvision\ndevice \u003d \u0027cuda\u0027 if torch.cuda.is_available() else \u0027cpu\u0027\nif device \u003d\u003d \u0027cuda\u0027:\n    print(\u0027Using GPU.\u0027)\n    print(\u0027Memory Allocated: {}\u0027.format(torch.cuda.memory_allocated()))\n    print(\u0027Max Memory Allocated: {}\u0027.format(torch.cuda.max_memory_allocated()))\n    print(\u0027Memory Reserved: {}\u0027.format(torch.cuda.memory_reserved()))\nelse:\n    print(\u0027No useable GPUs found.  Using CPU.\u0027)\n\n# Read recipe inputs\ndata_input \u003d dataiku.Folder(\"loYoKVsM\")\ninput_path \u003d data_input.get_path()\nmodel_output \u003d dataiku.Folder(\"UrfVTR9K\")\noutput_path \u003d model_output.get_path()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Using GPU.\nMemory Allocated: 0\nMax Memory Allocated: 0\nMemory Reserved: 0\n"
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "code_folding": []
      },
      "source": [
        "from shutil import copyfile\n\nfrom PIL import Image\nimport os\nimport json\n\nimport detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\n# import some common libraries\nimport os\nimport json\n\n\n# import some common detectron2 utilities\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog\nfrom detectron2.structures import BoxMode\nfrom detectron2.data.datasets import register_coco_instances\nfrom detectron2.engine import DefaultTrainer\nfrom detectron2.config import get_cfg\n\njson_path \u003d \u0027\u0027\nfor f in os.listdir(input_path):\n    if f.endswith(\u0027.json\u0027):\n        json_path \u003d os.path.join(input_path, f)\n        break\n\n\nfor f in os.listdir(input_path):\n    if f.endswith(\u0027.json\u0027):\n        break\nwith open(os.path.join(input_path, f), \u0027r\u0027) as file:\n    data \u003d json.load(file)\nclasses \u003d []\nfor thing in data[\u0027categories\u0027]:\n    classes.append(thing[\u0027name\u0027])\n\n\nbackbone \u003d None#config[\u0027backbone\u0027]\nims_per_batch \u003d 4#int(config[\u0027ims_per_batch\u0027])\nepochs \u003d 10000#int(config[\u0027epochs\u0027])\nresume \u003d False#config[\u0027resume\u0027]\nbase_lr \u003d 0.0001#config[\u0027base_lr\u0027]\nnum_checkpoints \u003d 5#config[\u0027num_checkpoints\u0027]\nstep \u003d True#config[\u0027step\u0027]\n\nmy_dict \u003d {\u0027backbone\u0027: backbone,\n           \u0027ims_per_batch\u0027: ims_per_batch,\n           \u0027epochs\u0027: epochs,\n           \u0027resume\u0027: resume,\n           \u0027base_lr\u0027: base_lr,\n           \u0027num_checkpoints\u0027: num_checkpoints,\n           \u0027step\u0027: step}\n\n\n\n\n\n\nwith open(os.path.join(output_path, \u0027config.json\u0027), \u0027w\u0027) as outfile:\n    json.dump(my_dict, outfile)\n\nregister_coco_instances(\"train_data\", {}, json_path, input_path)\n#register_coco_instances(\"CarDamage_val\", {}, os.path.join(car_Damage_Data_path, \u0027COCO_train_annos.json\u0027), input_path)\nbackbone \u003d \u0027mask_rcnn_R_50_C4_1x.yaml\u0027\ncfg \u003d get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/\" + backbone))\ncfg.DATASETS.TRAIN \u003d (\"train_data\",)\ncfg.DATASETS.TEST \u003d (\"train_data\",)\ncfg.MODEL.WEIGHTS \u003d os.path.join(output_path, \u0027model_final.pth\u0027)\ncfg.SOLVER.IMS_PER_BATCH \u003d ims_per_batch\ncfg.SOLVER.BASE_LR \u003d base_lr\ncfg.SOLVER.MAX_ITER \u003d epochs\n#cfg.BATCHSIZE \u003d 4\ncfg.SOLVER.CHECKPOINT_PERIOD \u003d epochs//num_checkpoints\ncfg.MODEL.ROI_HEADS.NUM_CLASSES \u003d len(classes)\nif not step:\n    cfg.SOLVER.STEPS \u003d []        # do not decay learning rate\ncfg.OUTPUT_DIR \u003d output_path\ncfg.MODEL.DEVICE \u003d device\n\ntrainer \u003d DefaultTrainer(cfg)\nif not resume:\n    for f in os.listdir(output_path):\n        os.remove(os.path.join(output_path, f))\ntrainer.resume_or_load(resume\u003dresume)\ntrainer.train()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from PIL import Image\nimport os\nimport json\n\ncwd \u003d os.getcwd()\nos.mkdir(\u0027my_images\u0027)\nos.mkdir(\u0027my_model\u0027)\nprint(\u0027cwd: {}\u0027.format(cwd))\nprint(os.listdir())\n\n\nfor img in [item[\"fullPath\"] for item in data_input.get_path_details()[\"children\"]]:\n    with data_input.get_download_stream(path\u003dimg) as stream:\n        if img.endswith(\u0027.jpg\u0027):\n            im \u003d Image.open(stream)\n            im.save(cwd + \u0027/my_images\u0027 + img)\n        elif img.endswith(\u0027.json\u0027):\n            string \u003d stream.read().decode(\u0027utf-8\u0027)\n            json_obj \u003d json.loads(string)\n            with open(cwd + \u0027/my_images\u0027 + img, \u0027w\u0027) as f:\n                json.dump(json_obj, f)\n        else:\n            print(\u0027NON-FATAL ERROR: Not an image/json file, file type not supported.\u0027)\n\n\n\u0027\u0027\u0027for img in [item[\"fullPath\"] for item in model_input.get_path_details()[\"children\"]]:\n    with model_input.get_download_stream(path\u003dimg) as stream:\n        string \u003d stream.read()\u0027\u0027\u0027\n\nimport detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\n# import some common libraries\nimport os\nimport json\n\n\n# import some common detectron2 utilities\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog\nfrom detectron2.structures import BoxMode\nfrom detectron2.data.datasets import register_coco_instances\nfrom detectron2.engine import DefaultTrainer\nfrom detectron2.config import get_cfg\n\ninput_path \u003d \u0027my_images\u0027\noutput_path \u003d \u0027my_model\u0027\n\njson_path \u003d \u0027\u0027\nfor f in os.listdir(input_path):\n    if f.endswith(\u0027.json\u0027):\n        json_path \u003d os.path.join(input_path, f)\n        break\n\n\nfor f in os.listdir(input_path):\n    if f.endswith(\u0027.json\u0027):\n        break\nwith open(os.path.join(input_path, f), \u0027r\u0027) as file:\n    data \u003d json.load(file)\nclasses \u003d []\nfor thing in data[\u0027categories\u0027]:\n    classes.append(thing[\u0027name\u0027])\n\n\nbackbone \u003d None#config[\u0027backbone\u0027]\nims_per_batch \u003d 16#int(config[\u0027ims_per_batch\u0027])\nepochs \u003d 10000#int(config[\u0027epochs\u0027])\nresume \u003d False#config[\u0027resume\u0027]\nbase_lr \u003d 0.0001#config[\u0027base_lr\u0027]\nnum_checkpoints \u003d 5#config[\u0027num_checkpoints\u0027]\nstep \u003d True#config[\u0027step\u0027]\n\nmy_dict \u003d {\u0027backbone\u0027: backbone,\n           \u0027ims_per_batch\u0027: ims_per_batch,\n           \u0027epochs\u0027: epochs,\n           \u0027resume\u0027: resume,\n           \u0027base_lr\u0027: base_lr,\n           \u0027num_checkpoints\u0027: num_checkpoints,\n           \u0027step\u0027: step}\n\n\n\n\n\n\nwith open(os.path.join(output_path, \u0027config.json\u0027), \u0027w\u0027) as outfile:\n    json.dump(my_dict, outfile)\n\nregister_coco_instances(\"train_data\", {}, json_path, input_path)\n#register_coco_instances(\"CarDamage_val\", {}, os.path.join(car_Damage_Data_path, \u0027COCO_train_annos.json\u0027), input_path)\nif type(backbone)!\u003dstr:\n    print(\u0027wuht:\u0027)\n    print(backbone)\n    backbone \u003d \u0027mask_rcnn_R_50_C4_1x.yaml\u0027\ncfg \u003d get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/\" + backbone))\ncfg.DATASETS.TRAIN \u003d (\"train_data\",)\ncfg.DATASETS.TEST \u003d (\"train_data\",)\ncfg.MODEL.WEIGHTS \u003d model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/\" + backbone)\ncfg.SOLVER.IMS_PER_BATCH \u003d ims_per_batch\ncfg.INPUT.MIN_SIZE_TRAIN \u003d (400,)\ncfg.INPUT.MIN_SIZE_TEST \u003d (400,)\ncfg.SOLVER.BASE_LR \u003d base_lr\ncfg.SOLVER.MAX_ITER \u003d epochs\n#cfg.BATCHSIZE \u003d 4\ncfg.SOLVER.CHECKPOINT_PERIOD \u003d epochs//num_checkpoints\ncfg.MODEL.ROI_HEADS.NUM_CLASSES \u003d len(classes)\nif not step:\n    cfg.SOLVER.STEPS \u003d []        # do not decay learning rate\ncfg.OUTPUT_DIR \u003d output_path\ncfg.MODEL.DEVICE \u003d device\n\ntrainer \u003d DefaultTrainer(cfg)\nif not resume:\n    for f in os.listdir(output_path):\n        os.remove(os.path.join(output_path, f))\ntrainer.resume_or_load(resume\u003dresume)\ntrainer.train()\n\n\u0027\u0027\u0027\nimport detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\n# import some common libraries\nimport cv2\n\n# import some common detectron2 utilities\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\n\nimport os\nimport json\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import ColorMode\nfrom detectron2.data import MetadataCatalog\n\nwith open(model_path + \u0027/last_checkpoint\u0027, \u0027r\u0027) as f:\n    last_model \u003d f.readline()\n\nwith open(os.path.join(model_path, \u0027config.json\u0027), \u0027r\u0027) as f:\n    config_data \u003d json.load(f)\nbackbone \u003d config_data[\u0027backbone\u0027]\n\nfor f in os.listdir(data_path):\n    if f.endswith(\u0027.json\u0027):\n        break\nwith open(os.path.join(data_path, f), \u0027r\u0027) as file:\n    data \u003d json.load(file)\nclasses \u003d []\nfor thing in data[\u0027categories\u0027]:\n    classes.append(thing[\u0027name\u0027])\n\n\nMetadataCatalog.get(\u0027CarDamage_damage_type_metadata\u0027).set(thing_classes\u003dclasses)\nmetadata \u003d MetadataCatalog.get(\u0027CarDamage_damage_type_metadata\u0027)\n\n\ncfg \u003d get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(backbone))\ncfg.MODEL.WEIGHTS \u003d os.path.join(model_path, last_model)\ncfg.MODEL.ROI_HEADS.NUM_CLASSES \u003d len(classes)\ncfg.MODEL.DEVICE \u003d device\ncfg.OUTPUT_DIR \u003d output_path\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST \u003d threshold\npredictor \u003d DefaultPredictor(cfg)\n\n# get the frame width and height\n#output.write_json(\u0027dummy-file.txt\u0027, \u0027{}\u0027)\n\n\nvideo_path \u003d  input_path + config[\u0027video\u0027]\nfor file in os.listdir(input_path):\n    if not file.endswith(\u0027.json\u0027):\n        video_path \u003d os.path.join(input_path, file)\n\n        cap \u003d cv2.VideoCapture(video_path)\n        if (cap.isOpened() \u003d\u003d False):\n            print(\u0027Error while trying to read {}. Please check path again.\u0027.format(file))\n\n        threshold \u003d 0.90\n\n        frame_width \u003d int(cap.get(3))\n        frame_height \u003d int(cap.get(4))\n        output_name \u003d  output_path + \"/\" + file.split(\u0027.\u0027)[0] + \".mp4\"\n        # get fps (try and except for different cv2 versions)\n        try:\n            fps \u003d cap.get(cv2.cv.CV_CAP_PROP_FPS)\n        except:\n            fps \u003d cap.get(cv2.CAP_PROP_FPS)\n        # define codec and create VideoWriter object\n        out \u003d cv2.VideoWriter(output_name,\n                          cv2.VideoWriter_fourcc(*\u0027mp4v\u0027), fps,\n                          (frame_width, frame_height))\n        ret \u003d True\n        while ret:\n            # capture each frame of the video\n            ret, im \u003d cap.read()\n            if ret \u003d\u003d True:\n                # keep a copy of the original image for OpenCV functions and applying masks\n                if image_background:\n                    orig_image \u003d image.copy()\n                else:\n                    orig_image \u003d np.zeros_like(image)\n                outputs \u003d predictor(im)\n                v \u003d Visualizer(im[:, :, ::-1],\n                               metadata\u003dmetadata,\n                               scale\u003d1,\n                               instance_mode\u003dColorMode.IMAGE   # remove the colors of unsegmented pixels\n                )\n                v \u003d v.draw_instance_predictions(outputs[\"instances\"].to(device))\n                new_im \u003d v.get_image()[:, :, ::-1]\n                out.write(new_im)\n        with video_output.get_writer(file) as w:\n            w.write()\n\n\n            \u0027\u0027\u0027\n\n\n\n\n#os.remove(os.path.join(output_path, \u0027dummy-file.txt\u0027))"
      ],
      "outputs": []
    }
  ]
}