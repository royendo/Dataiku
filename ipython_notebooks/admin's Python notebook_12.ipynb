{
  "metadata": {
    "kernelspec": {
      "display_name": "Python (env 30688)",
      "name": "py-dku-venv-30688",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "version": "3.6.9",
      "name": "python",
      "pygments_lexer": "ipython3",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "creator": "admin",
    "createdOn": 1646031796039,
    "hide_input": false,
    "modifiedBy": "admin",
    "customFields": {},
    "tags": []
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pylab inline"
      ],
      "outputs": []
    },
    {
      "execution_count": 2,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\nimport torch\nfrom keybert import KeyBERT\ndoc \u003d \"\"\"\n         Supervised learning is the machine learning task of learning a function that\n         maps an input to an output based on example input-output pairs.[1] It infers a\n         function from labeled training data consisting of a set of training examples.[2]\n         In supervised learning, each example is a pair consisting of an input object\n         (typically a vector) and a desired output value (also called the supervisory signal).\n         A supervised learning algorithm analyzes the training data and produces an inferred function,\n         which can be used for mapping new examples. An optimal scenario will allow for the\n         algorithm to correctly determine the class labels for unseen instances. This requires\n         the learning algorithm to generalize from the training data to unseen situations in a\n         \u0027reasonable\u0027 way (see inductive bias).\n      \"\"\"\nkw_model \u003d KeyBERT()\nkeywords \u003d kw_model.extract_keywords(doc)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 3,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(keywords)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[(\u0027supervised\u0027, 0.6523), (\u0027labeled\u0027, 0.4702), (\u0027learning\u0027, 0.467), (\u0027training\u0027, 0.3858), (\u0027labels\u0027, 0.3728)]\n"
        }
      ]
    },
    {
      "execution_count": 4,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "a \u003d \"/web-apps-backends/white_spot_pipeline/VIqeKRTH/\""
      ],
      "outputs": []
    },
    {
      "execution_count": 7,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "split \u003d a.split(\"/\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 9,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "split[-2]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "metadata": {},
          "data": {
            "text/plain": "\u0027VIqeKRTH\u0027"
          },
          "execution_count": 9
        }
      ]
    },
    {
      "execution_count": 4,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from dataiku.scenario import Trigger\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\nrun \u003d dataiku.Dataset(\"testdates\")\nrun_df \u003d run.get_dataframe()\n#print(run_df[\u0027line\u0027].values[0])\nif run_df[\u0027line\u0027].values[0] \u003d\u003d \u00272017-01-01\u0027:\n    print(\"X\")\nelse:\n    print(\"Y\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2017-01-01\nX\n"
        }
      ]
    },
    {
      "execution_count": 12,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nproj\u003dclient.get_project(project_key)\n#progress_callback(1)\nprev_recipe_list\u003dproj.list_recipes()\n\n\nfor r in prev_recipe_list:  \n    if r[\u0027type\u0027].lower()\u003d\u003d \u0027join\u0027 or \u0027grouping\u0027:\n        rc\u003dproj.get_recipe(r[\u0027name\u0027])\n\n        #rc_def\u003drc.get_definition_and_payload()\n        #payload_str\u003drc_def.get_payload().encode(\u0027utf-8\u0027)\n\n\n        settings \u003d rc.get_settings()\n\n        recipe_raw \u003d settings.get_recipe_raw_definition()\n        try:\n            print(recipe_raw[\u0027params\u0027])\n        except:\n            print(recipe_raw[\u0027name\u0027])\n        settings.save()\n\n\n        #code_creator\u003ddiku.CodeRecipeCreator(r[\u0027name\u0027],\u0027spark\u0027,proj)\n        #code_creator\u003dself.set_recipe_io(code_creator,rc_def.get_recipe_inputs(),\u0027input\u0027)\n        #code_creator\u003dself.set_recipe_io(code_creator,rc_def.get_recipe_outputs(),\u0027output\u0027)\n        #code_creator\u003dcode_creator.with_script(payload_str)\n\n        #rc.delete()\n        #code_creator.build()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027clearOutputPartition\u0027: False}\n{\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027EXPLICIT_ENV\u0027, \u0027envName\u0027: \u002725088\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}, \u0027skipPrerunValidate\u0027: False}\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u002725287\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027customConfig\u0027: {}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}}\n{\u0027customConfig\u0027: {\u0027frequency_unit\u0027: \u0027D\u0027, \u0027frequency_end_of_week\u0027: \u0027SUN\u0027, \u0027frequency_step_hours\u0027: 1, \u0027frequency_step_minutes\u0027: 1, \u0027additional_columns\u0027: False, \u0027sampling_method\u0027: \u0027last_records\u0027, \u0027number_records\u0027: 10000, \u0027prediction_length\u0027: 1, \u0027forecasting_style\u0027: \u0027auto\u0027, \u0027season_length_min\u0027: 1, \u0027season_length_H\u0027: 24, \u0027season_length_D\u0027: 7, \u0027season_length_B\u0027: 5, \u0027season_length_W\u0027: 52, \u0027season_length_M\u0027: 12, \u0027season_length_3M\u0027: 4, \u0027season_length_6M\u0027: 2, \u0027season_length_12M\u0027: 1, \u0027trivial_identity_model_activated\u0027: True, \u0027seasonal_naive_model_activated\u0027: True, \u0027autoarima_model_activated\u0027: False, \u0027autoarima_model_kwargs\u0027: {\u0027seasonal\u0027: \u0027True\u0027}, \u0027seasonal_trend_model_activated\u0027: False, \u0027seasonal_trend_model_kwargs\u0027: {\u0027model\u0027: \u0027ETSModel\u0027}, \u0027npts_model_activated\u0027: False, \u0027npts_model_kwargs\u0027: {\u0027use_seasonal_model\u0027: \u0027True\u0027}, \u0027simplefeedforward_model_activated\u0027: True, \u0027simplefeedforward_model_kwargs\u0027: {\u0027num_hidden_dimensions\u0027: \u0027[40, 40]\u0027}, \u0027deepar_model_activated\u0027: True, \u0027deepar_model_kwargs\u0027: {\u0027num_layers\u0027: 2}, \u0027transformer_model_activated\u0027: False, \u0027transformer_model_kwargs\u0027: {\u0027model_dim\u0027: 32}, \u0027mqcnn_model_activated\u0027: False, \u0027mqcnn_model_kwargs\u0027: {}, \u0027tft_model_activated\u0027: False, \u0027tft_model_kwargs\u0027: {}, \u0027nbeats_model_activated\u0027: False, \u0027nbeats_model_kwargs\u0027: {}, \u0027epoch\u0027: 10, \u0027batch_size\u0027: 32, \u0027auto_num_batches_per_epoch\u0027: False, \u0027num_batches_per_epoch\u0027: 50, \u0027evaluation_strategy\u0027: \u0027split\u0027, \u0027external_feature_activated\u0027: False, \u0027evaluation_only\u0027: False, \u0027use_gpu\u0027: False, \u0027gpu_location\u0027: \u0027local_gpu\u0027, \u0027target_columns\u0027: [], \u0027timeseries_identifiers\u0027: [], \u0027external_feature_columns\u0027: [], \u0027gpu_devices\u0027: [], \u0027time_column\u0027: \u0027date_fixed\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}}\n{\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027INHERIT\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}, \u0027skipPrerunValidate\u0027: False}\n{\u0027autoInferSchema\u0027: False, \u0027expectHeaders\u0027: False, \u0027sendHeaders\u0027: False, \u0027skipPrerunValidate\u0027: False}\n{\u0027selection\u0027: {\u0027useMemTable\u0027: False, \u0027filter\u0027: {\u0027distinct\u0027: False, \u0027enabled\u0027: False}, \u0027partitionSelectionMethod\u0027: \u0027ALL\u0027, \u0027latestPartitionsN\u0027: 1, \u0027ordering\u0027: {\u0027enabled\u0027: False, \u0027rules\u0027: []}, \u0027samplingMethod\u0027: \u0027HEAD_SEQUENTIAL\u0027, \u0027maxRecords\u0027: 100000, \u0027targetRatio\u0027: 0.02, \u0027withinFirstN\u0027: -1, \u0027maxReadUncompressedBytes\u0027: -1}, \u0027engineParams\u0027: {\u0027dssEngineMaxThreads\u0027: 4, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027Corona1\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027Corona1\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\ncompute_Corona_PostgreSQL_joined\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027Corona_PostgreSQL\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027allowMultipleConnections\u0027: False, \u0027displayExecutionPlan\u0027: True, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027skipPrerunValidate\u0027: False}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027Corona_copy\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027Corona\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027engineType\u0027: \u0027SPARK\u0027, \u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027DKU_TUTORIAL_BASICS_101_WINE_QUALITY__2__COPY\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027selection\u0027: {\u0027useMemTable\u0027: False, \u0027filter\u0027: {\u0027distinct\u0027: False, \u0027enabled\u0027: False}, \u0027partitionSelectionMethod\u0027: \u0027ALL\u0027, \u0027latestPartitionsN\u0027: 1, \u0027ordering\u0027: {\u0027enabled\u0027: False, \u0027rules\u0027: []}, \u0027samplingMethod\u0027: \u0027HEAD_SEQUENTIAL\u0027, \u0027maxRecords\u0027: 100000, \u0027targetRatio\u0027: 0.02, \u0027withinFirstN\u0027: -1, \u0027maxReadUncompressedBytes\u0027: -1}, \u0027engineParams\u0027: {\u0027dssEngineMaxThreads\u0027: 4, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027DaysofYear\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\ncompute_DaysofYear_topn\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027EVERYTHING_aaaaa_1\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027EVERYTHING_aaaaa\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027selection\u0027: {\u0027useMemTable\u0027: False, \u0027filter\u0027: {\u0027distinct\u0027: False, \u0027enabled\u0027: False}, \u0027partitionSelectionMethod\u0027: \u0027ALL\u0027, \u0027latestPartitionsN\u0027: 1, \u0027ordering\u0027: {\u0027enabled\u0027: False, \u0027rules\u0027: []}, \u0027samplingMethod\u0027: \u0027HEAD_SEQUENTIAL\u0027, \u0027maxRecords\u0027: 100000, \u0027targetRatio\u0027: 0.02, \u0027withinFirstN\u0027: -1, \u0027maxReadUncompressedBytes\u0027: -1}, \u0027engineParams\u0027: {\u0027dssEngineMaxThreads\u0027: 4, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027FITNESS\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027INHERIT\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}, \u0027skipPrerunValidate\u0027: False}\n{\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027INHERIT\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}, \u0027skipPrerunValidate\u0027: False}\n{\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027INHERIT\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}, \u0027skipPrerunValidate\u0027: False}\n{\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027EXPLICIT_ENV\u0027, \u0027envName\u0027: \u002725088\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}, \u0027skipPrerunValidate\u0027: False}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\ncompute_KcAhzFTk_1_joined\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027INHERIT\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}, \u0027skipPrerunValidate\u0027: False}\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027greenplum_IN_prepared\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027schemaMode\u0027: \u0027FREE_SCHEMA_NAME_BASED\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027INHERIT\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}, \u0027skipPrerunValidate\u0027: False}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\ncompute_Untitled_spreadsheet_3_newsheet1_stacked\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027Untitled_spreadsheet__1_\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027selection\u0027: {\u0027useMemTable\u0027: False, \u0027filter\u0027: {\u0027distinct\u0027: False, \u0027enabled\u0027: False}, \u0027partitionSelectionMethod\u0027: \u0027ALL\u0027, \u0027latestPartitionsN\u0027: 1, \u0027ordering\u0027: {\u0027enabled\u0027: False, \u0027rules\u0027: []}, \u0027samplingMethod\u0027: \u0027HEAD_SEQUENTIAL\u0027, \u0027maxRecords\u0027: 100000, \u0027targetRatio\u0027: 0.02, \u0027withinFirstN\u0027: -1, \u0027maxReadUncompressedBytes\u0027: -1}, \u0027engineParams\u0027: {\u0027dssEngineMaxThreads\u0027: 4, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027INHERIT\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}, \u0027skipPrerunValidate\u0027: False}\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\ncompute_aaa123_copys3_joined\n{\u0027allowMultipleConnections\u0027: False, \u0027displayExecutionPlan\u0027: True, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027skipPrerunValidate\u0027: False}\n{\u0027customConfig\u0027: {\u0027use_sender_value\u0027: False, \u0027use_subject_value\u0027: False, \u0027body_encoding\u0027: \u0027us-ascii\u0027, \u0027use_body_value\u0027: False, \u0027attachment_type\u0027: \u0027csv\u0027, \u0027smtp_host\u0027: \u0027smtp.gmail.com\u0027, \u0027smtp_port\u0027: 587, \u0027recipient_column\u0027: \u0027m.royendo@gmail.com\u0027, \u0027sender_column\u0027: \u0027m.royendo@gmail.com\u0027, \u0027subject_column\u0027: \u0027Roy\u0027, \u0027body_column\u0027: \u0027Roy\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}}\n{\u0027customConfig\u0027: {}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}}\n{\u0027customConfig\u0027: {\u0027time_reference_type\u0027: \u0027fixed_timestamp\u0027, \u0027input_features_definition\u0027: \u0027auto\u0027, \u0027use_temporal_window\u0027: False, \u0027num_windows_per_type\u0027: 1, \u0027whole_history_window_enabled\u0027: False, \u0027advance_activate\u0027: False, \u0027encoding_feature\u0027: False, \u0027population_definition_method\u0027: \u0027whole_dataset\u0027, \u0027num_populations\u0027: 0, \u0027manual_populations_dict_11\u0027: {}, \u0027manual_populations_dict_12\u0027: {}, \u0027manual_populations_dict_13\u0027: {}, \u0027manual_populations_dict_14\u0027: {}, \u0027manual_populations_dict_15\u0027: {}, \u0027brute_force_populations_dict\u0027: {}, \u0027aggregation_keys\u0027: [\u0027\u0027], \u0027numerical_columns\u0027: [], \u0027categorical_columns\u0027: [], \u0027timestamp_column\u0027: \u0027\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}}\n{\u0027allowMultipleConnections\u0027: False, \u0027displayExecutionPlan\u0027: True, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027skipPrerunValidate\u0027: False}\n{\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027INHERIT\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}, \u0027skipPrerunValidate\u0027: False}\n{\u0027customConfig\u0027: {}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}}\ncompute_audit_by_severity\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027customConfig\u0027: {}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}}\ncompute_community_by_id\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027community\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\ncompute_community_windows\n{\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027INHERIT\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}, \u0027skipPrerunValidate\u0027: False}\n{\u0027allowMultipleConnections\u0027: False, \u0027usePsql\u0027: True, \u0027inferOutputDatasetsSchema\u0027: True, \u0027statementsParsingMode\u0027: \u0027SPLIT\u0027, \u0027skipPrerunValidate\u0027: False}\n{\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027INHERIT\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}, \u0027skipPrerunValidate\u0027: False}\n{\u0027allowMultipleConnections\u0027: False, \u0027displayExecutionPlan\u0027: True, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027skipPrerunValidate\u0027: False}\n{\u0027allowMultipleConnections\u0027: False, \u0027displayExecutionPlan\u0027: True, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027skipPrerunValidate\u0027: False}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027greenplum_IN\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027customConfig\u0027: {}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}}\n{\u0027allowMultipleConnections\u0027: True, \u0027mainConnectionDataset\u0027: \u0027no_input_pgsql\u0027, \u0027displayExecutionPlan\u0027: True, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027skipPrerunValidate\u0027: False}\n"
        },
        {
          "output_type": "error",
          "evalue": "java.lang.IllegalArgumentException: The dataset no_input_pgsql is not among the inputs nor output of the recipe.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/opt/dataiku/install/dataiku-dss-9.0.5-osx/python/dataikuapi/dssclient.py\u001b[0m in \u001b[0;36m_perform_http\u001b[0;34m(self, method, path, params, body, stream, files, raw_body)\u001b[0m\n\u001b[1;32m   1048\u001b[0m                     stream \u003d stream)\n\u001b[0;32m-\u003e 1049\u001b[0;31m             \u001b[0mhttp_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhttp_res\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Library/DataScienceStudio/dss_home/code-envs/python/30688/lib/python3.6/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 940\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: http://127.0.0.1:11201/dip/publicapi/projects/EVERYTHING/recipes/compute_no_input",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mDataikuException\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-12-45ffe9431759\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecipe_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\u0027name\u0027\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 25\u001b[0;31m         \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/dataiku/install/dataiku-dss-9.0.5-osx/python/dataikuapi/dss/recipe.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m         return self.recipe.client._perform_json(\n\u001b[1;32m    298\u001b[0m                 \u001b[0;34m\"PUT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/projects/%s/recipes/%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecipe_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 299\u001b[0;31m                 body\u003dself.data)\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/dataiku/install/dataiku-dss-9.0.5-osx/python/dataikuapi/dssclient.py\u001b[0m in \u001b[0;36m_perform_json\u001b[0;34m(self, method, path, params, body, files, raw_body)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_perform_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_body\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1065\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_perform_http\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mparams\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_body\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mraw_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_perform_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_body\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/dataiku/install/dataiku-dss-9.0.5-osx/python/dataikuapi/dssclient.py\u001b[0m in \u001b[0;36m_perform_http\u001b[0;34m(self, method, path, params, body, stream, files, raw_body)\u001b[0m\n\u001b[1;32m   1054\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m                 \u001b[0mex\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"message\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttp_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1056\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mDataikuException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"errorType\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Unknown error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"message\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No message\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_perform_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_body\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDataikuException\u001b[0m: java.lang.IllegalArgumentException: The dataset no_input_pgsql is not among the inputs nor output of the recipe."
          ],
          "ename": "DataikuException"
        }
      ]
    },
    {
      "execution_count": 1,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!whoami"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "rendo\r\n"
        }
      ]
    },
    {
      "execution_count": 2,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pwd"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "/Users/rendo/Library/DataScienceStudio/dss_home/jupyter-run/dku-workdirs/EVERYTHING/admin_s_Python_notebook_128557ccf3\r\n"
        }
      ]
    },
    {
      "execution_count": 1,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\n\nproject_key\u003d\"EVERYTHING\"\nclient\u003ddataiku.api_client()\nfutures \u003d client.list_futures()\nprint(futures)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/Users/rendo/Library/DataScienceStudio/dss_home/code-envs/python/30688/lib/python3.6/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n  warnings.warn(msg)\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[{\u0027hasResult\u0027: True, \u0027aborted\u0027: False, \u0027alive\u0027: False, \u0027startTime\u0027: 1646793899745, \u0027runningTime\u0027: 4245, \u0027unknown\u0027: False, \u0027jobId\u0027: \u0027SEoWDwgW\u0027, \u0027jobDisplayName\u0027: \u0027Starting web app backend webapp_a26310_a10\u0027, \u0027payload\u0027: {\u0027targets\u0027: [{\u0027objectType\u0027: \u0027WEB_APP\u0027, \u0027projectKey\u0027: \u0027EVERYTHING\u0027, \u0027objectId\u0027: \u0027Qp1UwWO\u0027, \u0027name\u0027: \u0027test\u0027}], \u0027displayName\u0027: \u0027Starting web app backend webapp_a26310_a10\u0027, \u0027extras\u0027: {}}, \u0027progress\u0027: {\u0027states\u0027: []}, \u0027owner\u0027: \u0027admin\u0027}, {\u0027hasResult\u0027: False, \u0027aborted\u0027: False, \u0027alive\u0027: True, \u0027startTime\u0027: 1646793899726, \u0027runningTime\u0027: 7856818, \u0027unknown\u0027: False, \u0027jobId\u0027: \u0027Q3saKjXb\u0027, \u0027jobDisplayName\u0027: \u0027Backend for Web app\u0027, \u0027payload\u0027: {\u0027targets\u0027: [{\u0027objectType\u0027: \u0027WEB_APP\u0027, \u0027projectKey\u0027: \u0027EVERYTHING\u0027, \u0027objectId\u0027: \u0027Qp1UwWO\u0027, \u0027name\u0027: \u0027test\u0027}], \u0027displayName\u0027: \u0027Backend for Web app\u0027, \u0027extras\u0027: {\u0027crashCount\u0027: 0, \u0027pid\u0027: 97787}}, \u0027progress\u0027: {\u0027states\u0027: []}, \u0027owner\u0027: \u0027admin\u0027}]\n"
        }
      ]
    },
    {
      "execution_count": 4,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "project \u003d client.get_project(\"EVERYTHING\")\nfor val in project.list_jobs():\n    if val[\u0027state\u0027] \u003d\u003d running:\n        xyz"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "DONE\nABORTED\nDONE\nABORTED\nDONE\nDONE\nFAILED\nDONE\nDONE\nDONE\nDONE\nDONE\nDONE\nFAILED\nDONE\nDONE\nDONE\nDONE\nFAILED\nFAILED\nDONE\nFAILED\nDONE\nFAILED\nFAILED\nDONE\nDONE\nDONE\nDONE\nABORTED\nDONE\nDONE\nDONE\nDONE\nDONE\nDONE\nDONE\nDONE\nDONE\nDONE\nABORTED\nFAILED\nDONE\nDONE\nDONE\nDONE\nDONE\nDONE\nDONE\nDONE\nDONE\nDONE\nDONE\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nABORTED\nDONE\nDONE\nFAILED\nFAILED\nDONE\nFAILED\nDONE\nDONE\nDONE\nFAILED\nDONE\nFAILED\nDONE\nFAILED\nDONE\nFAILED\nDONE\nDONE\nDONE\nDONE\nDONE\nFAILED\nDONE\nDONE\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\n"
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nclient \u003d dataiku.api_client()\nplugin \u003d client.get_plugin(\"A26310\")\nplugin.update_from_git(\"https://bitbucket.wdc.com/scm/daosm/afl4g_python.git\", checkout\u003d\"master\")"
      ],
      "outputs": []
    }
  ]
}