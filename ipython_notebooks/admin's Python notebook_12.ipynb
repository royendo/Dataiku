{
  "metadata": {
    "kernelspec": {
      "display_name": "Python (env 30688)",
      "name": "py-dku-venv-30688",
      "language": "python"
    },
    "modifiedBy": "admin",
    "creator": "admin",
    "createdOn": 1646031796039,
    "hide_input": false,
    "customFields": {},
    "tags": []
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pylab inline"
      ],
      "outputs": []
    },
    {
      "execution_count": 2,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\nimport torch\nfrom keybert import KeyBERT\ndoc \u003d \"\"\"\n         Supervised learning is the machine learning task of learning a function that\n         maps an input to an output based on example input-output pairs.[1] It infers a\n         function from labeled training data consisting of a set of training examples.[2]\n         In supervised learning, each example is a pair consisting of an input object\n         (typically a vector) and a desired output value (also called the supervisory signal).\n         A supervised learning algorithm analyzes the training data and produces an inferred function,\n         which can be used for mapping new examples. An optimal scenario will allow for the\n         algorithm to correctly determine the class labels for unseen instances. This requires\n         the learning algorithm to generalize from the training data to unseen situations in a\n         \u0027reasonable\u0027 way (see inductive bias).\n      \"\"\"\nkw_model \u003d KeyBERT()\nkeywords \u003d kw_model.extract_keywords(doc)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 3,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(keywords)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[(\u0027supervised\u0027, 0.6523), (\u0027labeled\u0027, 0.4702), (\u0027learning\u0027, 0.467), (\u0027training\u0027, 0.3858), (\u0027labels\u0027, 0.3728)]\n"
        }
      ]
    },
    {
      "execution_count": 4,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "a \u003d \"/web-apps-backends/white_spot_pipeline/VIqeKRTH/\""
      ],
      "outputs": []
    },
    {
      "execution_count": 7,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "split \u003d a.split(\"/\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 9,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "split[-2]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "metadata": {},
          "data": {
            "text/plain": "\u0027VIqeKRTH\u0027"
          },
          "execution_count": 9
        }
      ]
    },
    {
      "execution_count": 4,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from dataiku.scenario import Trigger\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\nrun \u003d dataiku.Dataset(\"testdates\")\nrun_df \u003d run.get_dataframe()\n#print(run_df[\u0027line\u0027].values[0])\nif run_df[\u0027line\u0027].values[0] \u003d\u003d \u00272017-01-01\u0027:\n    print(\"X\")\nelse:\n    print(\"Y\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2017-01-01\nX\n"
        }
      ]
    },
    {
      "execution_count": 12,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nproj\u003dclient.get_project(project_key)\n#progress_callback(1)\nprev_recipe_list\u003dproj.list_recipes()\n\n\nfor r in prev_recipe_list:  \n    if r[\u0027type\u0027].lower()\u003d\u003d \u0027join\u0027 or \u0027grouping\u0027:\n        rc\u003dproj.get_recipe(r[\u0027name\u0027])\n\n        #rc_def\u003drc.get_definition_and_payload()\n        #payload_str\u003drc_def.get_payload().encode(\u0027utf-8\u0027)\n\n\n        settings \u003d rc.get_settings()\n\n        recipe_raw \u003d settings.get_recipe_raw_definition()\n        try:\n            print(recipe_raw[\u0027params\u0027])\n        except:\n            print(recipe_raw[\u0027name\u0027])\n        settings.save()\n\n\n        #code_creator\u003ddiku.CodeRecipeCreator(r[\u0027name\u0027],\u0027spark\u0027,proj)\n        #code_creator\u003dself.set_recipe_io(code_creator,rc_def.get_recipe_inputs(),\u0027input\u0027)\n        #code_creator\u003dself.set_recipe_io(code_creator,rc_def.get_recipe_outputs(),\u0027output\u0027)\n        #code_creator\u003dcode_creator.with_script(payload_str)\n\n        #rc.delete()\n        #code_creator.build()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027clearOutputPartition\u0027: False}\n{\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027EXPLICIT_ENV\u0027, \u0027envName\u0027: \u002725088\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}, \u0027skipPrerunValidate\u0027: False}\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u002725287\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027customConfig\u0027: {}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}}\n{\u0027customConfig\u0027: {\u0027frequency_unit\u0027: \u0027D\u0027, \u0027frequency_end_of_week\u0027: \u0027SUN\u0027, \u0027frequency_step_hours\u0027: 1, \u0027frequency_step_minutes\u0027: 1, \u0027additional_columns\u0027: False, \u0027sampling_method\u0027: \u0027last_records\u0027, \u0027number_records\u0027: 10000, \u0027prediction_length\u0027: 1, \u0027forecasting_style\u0027: \u0027auto\u0027, \u0027season_length_min\u0027: 1, \u0027season_length_H\u0027: 24, \u0027season_length_D\u0027: 7, \u0027season_length_B\u0027: 5, \u0027season_length_W\u0027: 52, \u0027season_length_M\u0027: 12, \u0027season_length_3M\u0027: 4, \u0027season_length_6M\u0027: 2, \u0027season_length_12M\u0027: 1, \u0027trivial_identity_model_activated\u0027: True, \u0027seasonal_naive_model_activated\u0027: True, \u0027autoarima_model_activated\u0027: False, \u0027autoarima_model_kwargs\u0027: {\u0027seasonal\u0027: \u0027True\u0027}, \u0027seasonal_trend_model_activated\u0027: False, \u0027seasonal_trend_model_kwargs\u0027: {\u0027model\u0027: \u0027ETSModel\u0027}, \u0027npts_model_activated\u0027: False, \u0027npts_model_kwargs\u0027: {\u0027use_seasonal_model\u0027: \u0027True\u0027}, \u0027simplefeedforward_model_activated\u0027: True, \u0027simplefeedforward_model_kwargs\u0027: {\u0027num_hidden_dimensions\u0027: \u0027[40, 40]\u0027}, \u0027deepar_model_activated\u0027: True, \u0027deepar_model_kwargs\u0027: {\u0027num_layers\u0027: 2}, \u0027transformer_model_activated\u0027: False, \u0027transformer_model_kwargs\u0027: {\u0027model_dim\u0027: 32}, \u0027mqcnn_model_activated\u0027: False, \u0027mqcnn_model_kwargs\u0027: {}, \u0027tft_model_activated\u0027: False, \u0027tft_model_kwargs\u0027: {}, \u0027nbeats_model_activated\u0027: False, \u0027nbeats_model_kwargs\u0027: {}, \u0027epoch\u0027: 10, \u0027batch_size\u0027: 32, \u0027auto_num_batches_per_epoch\u0027: False, \u0027num_batches_per_epoch\u0027: 50, \u0027evaluation_strategy\u0027: \u0027split\u0027, \u0027external_feature_activated\u0027: False, \u0027evaluation_only\u0027: False, \u0027use_gpu\u0027: False, \u0027gpu_location\u0027: \u0027local_gpu\u0027, \u0027target_columns\u0027: [], \u0027timeseries_identifiers\u0027: [], \u0027external_feature_columns\u0027: [], \u0027gpu_devices\u0027: [], \u0027time_column\u0027: \u0027date_fixed\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}}\n{\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027INHERIT\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}, \u0027skipPrerunValidate\u0027: False}\n{\u0027autoInferSchema\u0027: False, \u0027expectHeaders\u0027: False, \u0027sendHeaders\u0027: False, \u0027skipPrerunValidate\u0027: False}\n{\u0027selection\u0027: {\u0027useMemTable\u0027: False, \u0027filter\u0027: {\u0027distinct\u0027: False, \u0027enabled\u0027: False}, \u0027partitionSelectionMethod\u0027: \u0027ALL\u0027, \u0027latestPartitionsN\u0027: 1, \u0027ordering\u0027: {\u0027enabled\u0027: False, \u0027rules\u0027: []}, \u0027samplingMethod\u0027: \u0027HEAD_SEQUENTIAL\u0027, \u0027maxRecords\u0027: 100000, \u0027targetRatio\u0027: 0.02, \u0027withinFirstN\u0027: -1, \u0027maxReadUncompressedBytes\u0027: -1}, \u0027engineParams\u0027: {\u0027dssEngineMaxThreads\u0027: 4, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027Corona1\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027Corona1\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\ncompute_Corona_PostgreSQL_joined\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027Corona_PostgreSQL\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027allowMultipleConnections\u0027: False, \u0027displayExecutionPlan\u0027: True, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027skipPrerunValidate\u0027: False}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027Corona_copy\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027Corona\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027engineType\u0027: \u0027SPARK\u0027, \u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027DKU_TUTORIAL_BASICS_101_WINE_QUALITY__2__COPY\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027selection\u0027: {\u0027useMemTable\u0027: False, \u0027filter\u0027: {\u0027distinct\u0027: False, \u0027enabled\u0027: False}, \u0027partitionSelectionMethod\u0027: \u0027ALL\u0027, \u0027latestPartitionsN\u0027: 1, \u0027ordering\u0027: {\u0027enabled\u0027: False, \u0027rules\u0027: []}, \u0027samplingMethod\u0027: \u0027HEAD_SEQUENTIAL\u0027, \u0027maxRecords\u0027: 100000, \u0027targetRatio\u0027: 0.02, \u0027withinFirstN\u0027: -1, \u0027maxReadUncompressedBytes\u0027: -1}, \u0027engineParams\u0027: {\u0027dssEngineMaxThreads\u0027: 4, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027DaysofYear\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\ncompute_DaysofYear_topn\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027EVERYTHING_aaaaa_1\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027EVERYTHING_aaaaa\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027selection\u0027: {\u0027useMemTable\u0027: False, \u0027filter\u0027: {\u0027distinct\u0027: False, \u0027enabled\u0027: False}, \u0027partitionSelectionMethod\u0027: \u0027ALL\u0027, \u0027latestPartitionsN\u0027: 1, \u0027ordering\u0027: {\u0027enabled\u0027: False, \u0027rules\u0027: []}, \u0027samplingMethod\u0027: \u0027HEAD_SEQUENTIAL\u0027, \u0027maxRecords\u0027: 100000, \u0027targetRatio\u0027: 0.02, \u0027withinFirstN\u0027: -1, \u0027maxReadUncompressedBytes\u0027: -1}, \u0027engineParams\u0027: {\u0027dssEngineMaxThreads\u0027: 4, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027FITNESS\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027INHERIT\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}, \u0027skipPrerunValidate\u0027: False}\n{\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027INHERIT\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}, \u0027skipPrerunValidate\u0027: False}\n{\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027INHERIT\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}, \u0027skipPrerunValidate\u0027: False}\n{\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027EXPLICIT_ENV\u0027, \u0027envName\u0027: \u002725088\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}, \u0027skipPrerunValidate\u0027: False}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\ncompute_KcAhzFTk_1_joined\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027INHERIT\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}, \u0027skipPrerunValidate\u0027: False}\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027greenplum_IN_prepared\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027schemaMode\u0027: \u0027FREE_SCHEMA_NAME_BASED\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027INHERIT\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}, \u0027skipPrerunValidate\u0027: False}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\ncompute_Untitled_spreadsheet_3_newsheet1_stacked\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027Untitled_spreadsheet__1_\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027selection\u0027: {\u0027useMemTable\u0027: False, \u0027filter\u0027: {\u0027distinct\u0027: False, \u0027enabled\u0027: False}, \u0027partitionSelectionMethod\u0027: \u0027ALL\u0027, \u0027latestPartitionsN\u0027: 1, \u0027ordering\u0027: {\u0027enabled\u0027: False, \u0027rules\u0027: []}, \u0027samplingMethod\u0027: \u0027HEAD_SEQUENTIAL\u0027, \u0027maxRecords\u0027: 100000, \u0027targetRatio\u0027: 0.02, \u0027withinFirstN\u0027: -1, \u0027maxReadUncompressedBytes\u0027: -1}, \u0027engineParams\u0027: {\u0027dssEngineMaxThreads\u0027: 4, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027INHERIT\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}, \u0027skipPrerunValidate\u0027: False}\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\ncompute_aaa123_copys3_joined\n{\u0027allowMultipleConnections\u0027: False, \u0027displayExecutionPlan\u0027: True, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027skipPrerunValidate\u0027: False}\n{\u0027customConfig\u0027: {\u0027use_sender_value\u0027: False, \u0027use_subject_value\u0027: False, \u0027body_encoding\u0027: \u0027us-ascii\u0027, \u0027use_body_value\u0027: False, \u0027attachment_type\u0027: \u0027csv\u0027, \u0027smtp_host\u0027: \u0027smtp.gmail.com\u0027, \u0027smtp_port\u0027: 587, \u0027recipient_column\u0027: \u0027m.royendo@gmail.com\u0027, \u0027sender_column\u0027: \u0027m.royendo@gmail.com\u0027, \u0027subject_column\u0027: \u0027Roy\u0027, \u0027body_column\u0027: \u0027Roy\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}}\n{\u0027customConfig\u0027: {}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}}\n{\u0027customConfig\u0027: {\u0027time_reference_type\u0027: \u0027fixed_timestamp\u0027, \u0027input_features_definition\u0027: \u0027auto\u0027, \u0027use_temporal_window\u0027: False, \u0027num_windows_per_type\u0027: 1, \u0027whole_history_window_enabled\u0027: False, \u0027advance_activate\u0027: False, \u0027encoding_feature\u0027: False, \u0027population_definition_method\u0027: \u0027whole_dataset\u0027, \u0027num_populations\u0027: 0, \u0027manual_populations_dict_11\u0027: {}, \u0027manual_populations_dict_12\u0027: {}, \u0027manual_populations_dict_13\u0027: {}, \u0027manual_populations_dict_14\u0027: {}, \u0027manual_populations_dict_15\u0027: {}, \u0027brute_force_populations_dict\u0027: {}, \u0027aggregation_keys\u0027: [\u0027\u0027], \u0027numerical_columns\u0027: [], \u0027categorical_columns\u0027: [], \u0027timestamp_column\u0027: \u0027\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}}\n{\u0027allowMultipleConnections\u0027: False, \u0027displayExecutionPlan\u0027: True, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027skipPrerunValidate\u0027: False}\n{\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027INHERIT\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}, \u0027skipPrerunValidate\u0027: False}\n{\u0027customConfig\u0027: {}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}}\ncompute_audit_by_severity\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027test\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027customConfig\u0027: {}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}}\ncompute_community_by_id\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027community\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\ncompute_community_windows\n{\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027INHERIT\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}, \u0027skipPrerunValidate\u0027: False}\n{\u0027allowMultipleConnections\u0027: False, \u0027usePsql\u0027: True, \u0027inferOutputDatasetsSchema\u0027: True, \u0027statementsParsingMode\u0027: \u0027SPLIT\u0027, \u0027skipPrerunValidate\u0027: False}\n{\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027INHERIT\u0027}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}, \u0027skipPrerunValidate\u0027: False}\n{\u0027allowMultipleConnections\u0027: False, \u0027displayExecutionPlan\u0027: True, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027skipPrerunValidate\u0027: False}\n{\u0027allowMultipleConnections\u0027: False, \u0027displayExecutionPlan\u0027: True, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027skipPrerunValidate\u0027: False}\n{\u0027schemaMode\u0027: \u0027STRICT_SYNC\u0027, \u0027forcePipelineableForTests\u0027: False, \u0027engineParams\u0027: {\u0027maxThreads\u0027: 4, \u0027tdchParams\u0027: {\u0027splitMode\u0027: \u0027DEFAULT\u0027, \u0027numberOfExecutors\u0027: 2}, \u0027sparkSQL\u0027: {\u0027overwriteOutputSchema\u0027: False, \u0027useGlobalMetastore\u0027: False, \u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {}}, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027lowerCaseSchemaIfEngineRequiresIt\u0027: False}}\n{\u0027engineParams\u0027: {\u0027spark\u0027: {\u0027readParams\u0027: {\u0027mode\u0027: \u0027AUTO\u0027, \u0027autoModeRepartitionInto\u0027: 10, \u0027map\u0027: {\u0027greenplum_IN\u0027: {\u0027repartition\u0027: 10}}}, \u0027useGlobalMetastore\u0027: False, \u0027useNativeProcessors\u0027: True, \u0027sparkConfig\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027conf\u0027: []}, \u0027executionEngine\u0027: \u0027SPARK_SUBMIT\u0027, \u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True, \u0027skipPrerunValidate\u0027: False}, \u0027hive\u0027: {\u0027inheritConf\u0027: \u0027default\u0027, \u0027hiveconf\u0027: [], \u0027executionEngine\u0027: \u0027HIVESERVER2\u0027, \u0027addDkuUdf\u0027: False, \u0027skipPrerunValidate\u0027: False}, \u0027impala\u0027: {\u0027forceStreamMode\u0027: True}, \u0027hadoopConfigKeys\u0027: [], \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027maxThreads\u0027: 8}}\n{\u0027customConfig\u0027: {}, \u0027containerSelection\u0027: {\u0027containerMode\u0027: \u0027INHERIT\u0027}}\n{\u0027allowMultipleConnections\u0027: True, \u0027mainConnectionDataset\u0027: \u0027no_input_pgsql\u0027, \u0027displayExecutionPlan\u0027: True, \u0027sqlPipelineParams\u0027: {\u0027pipelineAllowStart\u0027: True, \u0027pipelineAllowMerge\u0027: True}, \u0027skipPrerunValidate\u0027: False}\n"
        },
        {
          "output_type": "error",
          "evalue": "java.lang.IllegalArgumentException: The dataset no_input_pgsql is not among the inputs nor output of the recipe.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/opt/dataiku/install/dataiku-dss-9.0.5-osx/python/dataikuapi/dssclient.py\u001b[0m in \u001b[0;36m_perform_http\u001b[0;34m(self, method, path, params, body, stream, files, raw_body)\u001b[0m\n\u001b[1;32m   1048\u001b[0m                     stream \u003d stream)\n\u001b[0;32m-\u003e 1049\u001b[0;31m             \u001b[0mhttp_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhttp_res\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Library/DataScienceStudio/dss_home/code-envs/python/30688/lib/python3.6/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 940\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: http://127.0.0.1:11201/dip/publicapi/projects/EVERYTHING/recipes/compute_no_input",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mDataikuException\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-12-45ffe9431759\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecipe_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\u0027name\u0027\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 25\u001b[0;31m         \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/dataiku/install/dataiku-dss-9.0.5-osx/python/dataikuapi/dss/recipe.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m         return self.recipe.client._perform_json(\n\u001b[1;32m    298\u001b[0m                 \u001b[0;34m\"PUT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/projects/%s/recipes/%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecipe_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 299\u001b[0;31m                 body\u003dself.data)\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/dataiku/install/dataiku-dss-9.0.5-osx/python/dataikuapi/dssclient.py\u001b[0m in \u001b[0;36m_perform_json\u001b[0;34m(self, method, path, params, body, files, raw_body)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_perform_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_body\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1065\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_perform_http\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mparams\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_body\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mraw_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_perform_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_body\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/dataiku/install/dataiku-dss-9.0.5-osx/python/dataikuapi/dssclient.py\u001b[0m in \u001b[0;36m_perform_http\u001b[0;34m(self, method, path, params, body, stream, files, raw_body)\u001b[0m\n\u001b[1;32m   1054\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m                 \u001b[0mex\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"message\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttp_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1056\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mDataikuException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"errorType\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Unknown error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"message\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No message\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_perform_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_body\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDataikuException\u001b[0m: java.lang.IllegalArgumentException: The dataset no_input_pgsql is not among the inputs nor output of the recipe."
          ],
          "ename": "DataikuException"
        }
      ]
    },
    {
      "execution_count": 1,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!whoami"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "rendo\r\n"
        }
      ]
    },
    {
      "execution_count": 2,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pwd"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "/Users/rendo/Library/DataScienceStudio/dss_home/jupyter-run/dku-workdirs/EVERYTHING/admin_s_Python_notebook_128557ccf3\r\n"
        }
      ]
    },
    {
      "execution_count": 1,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/Users/rendo/Library/DataScienceStudio/dss_home/code-envs/python/30688/lib/python3.6/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n  warnings.warn(msg)\n"
        }
      ]
    },
    {
      "execution_count": 4,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "project \u003d client.get_project(\"EVERYTHING\")\nfor val in project.list_jobs():\n    if val[\u0027state\u0027] \u003d\u003d running:\n        xyz"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "DONE\nABORTED\nDONE\nABORTED\nDONE\nDONE\nFAILED\nDONE\nDONE\nDONE\nDONE\nDONE\nDONE\nFAILED\nDONE\nDONE\nDONE\nDONE\nFAILED\nFAILED\nDONE\nFAILED\nDONE\nFAILED\nFAILED\nDONE\nDONE\nDONE\nDONE\nABORTED\nDONE\nDONE\nDONE\nDONE\nDONE\nDONE\nDONE\nDONE\nDONE\nDONE\nABORTED\nFAILED\nDONE\nDONE\nDONE\nDONE\nDONE\nDONE\nDONE\nDONE\nDONE\nDONE\nDONE\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nABORTED\nDONE\nDONE\nFAILED\nFAILED\nDONE\nFAILED\nDONE\nDONE\nDONE\nFAILED\nDONE\nFAILED\nDONE\nFAILED\nDONE\nFAILED\nDONE\nDONE\nDONE\nDONE\nDONE\nFAILED\nDONE\nDONE\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\nFAILED\n"
        }
      ]
    },
    {
      "execution_count": 19,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\ndataiku.set_remote_dss(\"http://localhost:11200\",\u00279jpn2kh8gtZaWGU19TGvzvmwHmw0yP9z\u0027)\nclient \u003d dataiku.api_client()\nplugin \u003d client.get_plugin(\"a26310\")\nplugin.update_code_env()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "metadata": {},
          "data": {
            "text/plain": "\u003cdataikuapi.dss.future.DSSFuture at 0x11275a0f0\u003e"
          },
          "execution_count": 19
        }
      ]
    },
    {
      "execution_count": 18,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(usages)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u003cdataikuapi.dss.plugin.DSSPluginUsages object at 0x112733390\u003e\n"
        }
      ]
    },
    {
      "execution_count": 20,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "future \u003d client.install_plugin_from_git(\"git@github.com:royendo/Dataiku.git\")\n"
      ],
      "outputs": [
        {
          "output_type": "error",
          "evalue": "com.dataiku.dip.exceptions.UnauthorizedException: Action forbidden, you are not admin",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/opt/dataiku/install/dataiku-dss-9.0.5-osx/python/dataikuapi/dssclient.py\u001b[0m in \u001b[0;36m_perform_http\u001b[0;34m(self, method, path, params, body, stream, files, raw_body)\u001b[0m\n\u001b[1;32m   1048\u001b[0m                     stream \u003d stream)\n\u001b[0;32m-\u003e 1049\u001b[0;31m             \u001b[0mhttp_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhttp_res\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Library/DataScienceStudio/dss_home/code-envs/python/30688/lib/python3.6/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 940\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: http://localhost:11200/dip/publicapi/plugins/actions/installFromGit",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mDataikuException\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-20-79c2ae83be74\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0mfuture\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstall_plugin_from_git\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"git@github.com:royendo/Dataiku.git\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/opt/dataiku/install/dataiku-dss-9.0.5-osx/python/dataikuapi/dssclient.py\u001b[0m in \u001b[0;36minstall_plugin_from_git\u001b[0;34m(self, repository_url, checkout, subpath)\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;34m\"gitRepositoryUrl\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrepository_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;34m\"gitCheckout\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mcheckout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 289\u001b[0;31m             \u001b[0;34m\"gitSubpath\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msubpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         })\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDSSFuture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"jobId\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/dataiku/install/dataiku-dss-9.0.5-osx/python/dataikuapi/dssclient.py\u001b[0m in \u001b[0;36m_perform_json\u001b[0;34m(self, method, path, params, body, files, raw_body)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_perform_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_body\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1065\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_perform_http\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mparams\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_body\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mraw_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_perform_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_body\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/dataiku/install/dataiku-dss-9.0.5-osx/python/dataikuapi/dssclient.py\u001b[0m in \u001b[0;36m_perform_http\u001b[0;34m(self, method, path, params, body, stream, files, raw_body)\u001b[0m\n\u001b[1;32m   1054\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m                 \u001b[0mex\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"message\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttp_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1056\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mDataikuException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"errorType\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Unknown error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"message\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No message\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_perform_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_body\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDataikuException\u001b[0m: com.dataiku.dip.exceptions.UnauthorizedException: Action forbidden, you are not admin"
          ],
          "ename": "DataikuException"
        }
      ]
    },
    {
      "execution_count": 28,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nimport pprint as pp\ndataiku.set_remote_dss(\"http://localhost:11200\",\u00279jpn2kh8gtZaWGU19TGvzvmwHmw0yP9z\u0027)\nclient \u003d dataiku.api_client()\nproject \u003d client.get_project(\"EVERYTHING\")\nscenario \u003d project.get_scenario(\"24811\")\n\nscenario.run_and_wait()"
      ],
      "outputs": [
        {
          "output_type": "error",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-28-fce918a16ef9\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mscenario\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scenario\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"24811\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 8\u001b[0;31m \u001b[0mscenario\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_and_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/opt/dataiku/install/dataiku-dss-9.0.5-osx/python/dataikuapi/dss/scenario.py\u001b[0m in \u001b[0;36mrun_and_wait\u001b[0;34m(self, params, no_fail)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtrigger_fire\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 36\u001b[0;31m         \u001b[0mscenario_run\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mtrigger_fire\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_scenario_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_fail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mwaiter\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mDSSScenarioRunWaiter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscenario_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrigger_fire\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_fail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/dataiku/install/dataiku-dss-9.0.5-osx/python/dataikuapi/dss/scenario.py\u001b[0m in \u001b[0;36mwait_for_scenario_run\u001b[0;34m(self, no_fail)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mDataikuException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Scenario run has been cancelled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0mscenario_run\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scenario_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 646\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscenario_run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt"
        }
      ]
    },
    {
      "execution_count": 25,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "last_runs \u003d scenario.get_last_runs(only_finished_runs\u003dTrue)\n\nif len(last_runs) \u003d\u003d 0:\n    raise Exception(\"The scenario never ran\")\n\nlast_run \u003d last_runs[0]"
      ],
      "outputs": []
    },
    {
      "execution_count": 29,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pp.pprint(last_run.get_details())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{\u0027scenarioRun\u0027: {\u0027clustersUsed\u0027: [],\n                 \u0027end\u0027: 1647907142297,\n                 \u0027reportersStates\u0027: [],\n                 \u0027result\u0027: {\u0027end\u0027: 1647907142295,\n                            \u0027login\u0027: \u0027admin\u0027,\n                            \u0027outcome\u0027: \u0027SUCCESS\u0027,\n                            \u0027start\u0027: 1647907085679,\n                            \u0027target\u0027: {\u0027projectKey\u0027: \u0027EVERYTHING\u0027,\n                                       \u0027scenarioId\u0027: \u002724811\u0027,\n                                       \u0027type\u0027: \u0027SCENARIO\u0027},\n                            \u0027type\u0027: \u0027SCENARIO_DONE\u0027},\n                 \u0027runAsUser\u0027: {\u0027authSource\u0027: \u0027USER_FROM_UI\u0027,\n                               \u0027realUserLogin\u0027: \u0027admin\u0027,\n                               \u0027userGroupLevelPermissions\u0027: {\u0027admin\u0027: True,\n                                                             \u0027canObtainAPITicketFromCookiesForGroupsRegex\u0027: \u0027\u0027,\n                                                             \u0027mayCreateActiveWebContent\u0027: True,\n                                                             \u0027mayCreateAuthenticatedConnections\u0027: True,\n                                                             \u0027mayCreateClusters\u0027: True,\n                                                             \u0027mayCreateCodeEnvs\u0027: True,\n                                                             \u0027mayCreateProjects\u0027: True,\n                                                             \u0027mayCreateProjectsFromDataikuApps\u0027: True,\n                                                             \u0027mayCreateProjectsFromMacros\u0027: True,\n                                                             \u0027mayCreateProjectsFromTemplates\u0027: True,\n                                                             \u0027mayCreatePublishedAPIServices\u0027: True,\n                                                             \u0027mayCreatePublishedProjects\u0027: True,\n                                                             \u0027mayDevelopPlugins\u0027: True,\n                                                             \u0027mayEditLibFolders\u0027: True,\n                                                             \u0027mayManageClusters\u0027: True,\n                                                             \u0027mayManageCodeEnvs\u0027: True,\n                                                             \u0027mayManageUDM\u0027: True,\n                                                             \u0027mayViewIndexedHiveConnections\u0027: True,\n                                                             \u0027mayWriteInRootProjectFolder\u0027: True,\n                                                             \u0027mayWriteSafeCode\u0027: True,\n                                                             \u0027mayWriteUnsafeCode\u0027: True},\n                               \u0027userGroups\u0027: [\u0027administrators\u0027],\n                               \u0027userProfile\u0027: \u0027DATA_SCIENTIST\u0027,\n                               \u0027via\u0027: [\u0027scenario-run: \u0027\n                                       \u0027scenario\u003dEVERYTHING.24811 \u0027\n                                       \u0027runId\u003d2022-03-22-08-58-05-103\u0027,\n                                       \u0027endoroy\u0027]},\n                 \u0027runId\u0027: \u00272022-03-22-08-58-05-103\u0027,\n                 \u0027scenario\u0027: {\u0027active\u0027: False,\n                              \u0027automationLocal\u0027: False,\n                              \u0027checklists\u0027: {\u0027checklists\u0027: []},\n                              \u0027customFields\u0027: {},\n                              \u0027delayedTriggersBehavior\u0027: {\u0027delayWhileRunning\u0027: True,\n                                                          \u0027squashDelayedTriggers\u0027: True,\n                                                          \u0027suppressTriggersWhileRunning\u0027: True},\n                              \u0027id\u0027: \u002724811\u0027,\n                              \u0027name\u0027: \u002724811\u0027,\n                              \u0027params\u0027: {\u0027steps\u0027: [{\u0027delayBetweenRetries\u0027: 10,\n                                                    \u0027id\u0027: \u0027build_0_true_d_CoronaSync\u0027,\n                                                    \u0027maxRetriesOnFail\u0027: 0,\n                                                    \u0027name\u0027: \u0027Step #1\u0027,\n                                                    \u0027params\u0027: {\u0027builds\u0027: [{\u0027itemId\u0027: \u0027CoronaSync\u0027,\n                                                                           \u0027partitionsSpec\u0027: \u0027\u0027,\n                                                                           \u0027type\u0027: \u0027DATASET\u0027}],\n                                                               \u0027jobType\u0027: \u0027RECURSIVE_BUILD\u0027,\n                                                               \u0027proceedOnFailure\u0027: False,\n                                                               \u0027refreshHiveMetastore\u0027: True},\n                                                    \u0027resetScenarioStatus\u0027: False,\n                                                    \u0027runConditionExpression\u0027: \u0027\u0027,\n                                                    \u0027runConditionStatuses\u0027: [\u0027SUCCESS\u0027,\n                                                                             \u0027WARNING\u0027],\n                                                    \u0027runConditionType\u0027: \u0027RUN_IF_STATUS_MATCH\u0027,\n                                                    \u0027type\u0027: \u0027build_flowitem\u0027},\n                                                   {\u0027delayBetweenRetries\u0027: 10,\n                                                    \u0027id\u0027: \u0027custom_python\u0027,\n                                                    \u0027maxRetriesOnFail\u0027: 0,\n                                                    \u0027name\u0027: \u0027${NOW}\u0027,\n                                                    \u0027params\u0027: {\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027INHERIT\u0027},\n                                                               \u0027proceedOnFailure\u0027: False},\n                                                    \u0027resetScenarioStatus\u0027: False,\n                                                    \u0027runConditionExpression\u0027: \u0027\u0027,\n                                                    \u0027runConditionStatuses\u0027: [\u0027SUCCESS\u0027,\n                                                                             \u0027WARNING\u0027],\n                                                    \u0027runConditionType\u0027: \u0027RUN_IF_STATUS_MATCH\u0027,\n                                                    \u0027type\u0027: \u0027custom_python\u0027}]},\n                              \u0027projectKey\u0027: \u0027EVERYTHING\u0027,\n                              \u0027reporters\u0027: [],\n                              \u0027tags\u0027: [],\n                              \u0027triggers\u0027: [],\n                              \u0027type\u0027: \u0027step_based\u0027,\n                              \u0027versionTag\u0027: {\u0027lastModifiedBy\u0027: {\u0027login\u0027: \u0027admin\u0027},\n                                             \u0027lastModifiedOn\u0027: 1629723482178,\n                                             \u0027versionNumber\u0027: 9}},\n                 \u0027start\u0027: 1647907085101,\n                 \u0027trigger\u0027: {\u0027cancelled\u0027: False,\n                             \u0027params\u0027: {},\n                             \u0027projectKey\u0027: \u0027EVERYTHING\u0027,\n                             \u0027runId\u0027: \u00272022-03-22-08-58-05-099\u0027,\n                             \u0027scenarioId\u0027: \u002724811\u0027,\n                             \u0027timestamp\u0027: 1647907085097,\n                             \u0027trigger\u0027: {\u0027active\u0027: False,\n                                         \u0027delay\u0027: 0,\n                                         \u0027id\u0027: \u0027manual\u0027,\n                                         \u0027name\u0027: \u0027API run\u0027,\n                                         \u0027type\u0027: \u0027manual\u0027},\n                             \u0027triggerAuthCtx\u0027: {\u0027apiKey\u0027: {\u0027createdBy\u0027: \u0027endoroy\u0027,\n                                                           \u0027createdOn\u0027: 1634627670649,\n                                                           \u0027id\u0027: \u0027G4vevSNNQAkyIT9B\u0027,\n                                                           \u0027key\u0027: \u00279jpn2kh8gtZaWGU19TGvzvmwHmw0yP9z\u0027,\n                                                           \u0027label\u0027: \u0027Personal \u0027\n                                                                    \u0027for \u0027\n                                                                    \u0027endoroy\u0027,\n                                                           \u0027user\u0027: \u0027endoroy\u0027},\n                                                \u0027authSource\u0027: \u0027PERSONAL_API_KEY\u0027,\n                                                \u0027realUserLogin\u0027: \u0027roy.endo@dataiku.com\u0027,\n                                                \u0027userGroupLevelPermissions\u0027: {\u0027admin\u0027: False,\n                                                                              \u0027canObtainAPITicketFromCookiesForGroupsRegex\u0027: \u0027\u0027,\n                                                                              \u0027mayCreateActiveWebContent\u0027: True,\n                                                                              \u0027mayCreateAuthenticatedConnections\u0027: True,\n                                                                              \u0027mayCreateClusters\u0027: False,\n                                                                              \u0027mayCreateCodeEnvs\u0027: True,\n                                                                              \u0027mayCreateProjects\u0027: True,\n                                                                              \u0027mayCreateProjectsFromDataikuApps\u0027: True,\n                                                                              \u0027mayCreateProjectsFromMacros\u0027: True,\n                                                                              \u0027mayCreateProjectsFromTemplates\u0027: True,\n                                                                              \u0027mayCreatePublishedAPIServices\u0027: True,\n                                                                              \u0027mayCreatePublishedProjects\u0027: True,\n                                                                              \u0027mayDevelopPlugins\u0027: True,\n                                                                              \u0027mayEditLibFolders\u0027: True,\n                                                                              \u0027mayManageClusters\u0027: False,\n                                                                              \u0027mayManageCodeEnvs\u0027: True,\n                                                                              \u0027mayManageUDM\u0027: True,\n                                                                              \u0027mayViewIndexedHiveConnections\u0027: True,\n                                                                              \u0027mayWriteInRootProjectFolder\u0027: True,\n                                                                              \u0027mayWriteSafeCode\u0027: True,\n                                                                              \u0027mayWriteUnsafeCode\u0027: True},\n                                                \u0027userGroups\u0027: [\u0027data_team\u0027],\n                                                \u0027userProfile\u0027: \u0027DATA_SCIENTIST\u0027,\n                                                \u0027via\u0027: []}},\n                 \u0027variables\u0027: {}},\n \u0027stepRuns\u0027: [{\u0027additionalReportItems\u0027: [{\u0027end\u0027: 1647907112362,\n                                          \u0027outcome\u0027: \u0027SUCCESS\u0027,\n                                          \u0027start\u0027: 1647907107798,\n                                          \u0027target\u0027: {\u0027datasetName\u0027: \u0027Corona_PostgreSQL\u0027,\n                                                     \u0027partition\u0027: \u0027NP\u0027,\n                                                     \u0027projectKey\u0027: \u0027EVERYTHING\u0027,\n                                                     \u0027type\u0027: \u0027DATASET_PARTITION\u0027},\n                                          \u0027type\u0027: \u0027BUILT_DATASET\u0027,\n                                          \u0027warnings\u0027: {\u0027totalCount\u0027: 0,\n                                                       \u0027warnings\u0027: {}}},\n                                         {\u0027end\u0027: 1647907139505,\n                                          \u0027outcome\u0027: \u0027SUCCESS\u0027,\n                                          \u0027start\u0027: 1647907112467,\n                                          \u0027target\u0027: {\u0027datasetName\u0027: \u0027CoronaSync\u0027,\n                                                     \u0027partition\u0027: \u0027NP\u0027,\n                                                     \u0027projectKey\u0027: \u0027EVERYTHING\u0027,\n                                                     \u0027type\u0027: \u0027DATASET_PARTITION\u0027},\n                                          \u0027type\u0027: \u0027BUILT_DATASET\u0027,\n                                          \u0027warnings\u0027: {\u0027totalCount\u0027: 0,\n                                                       \u0027warnings\u0027: {}}},\n                                         {\u0027end\u0027: 1647907142102,\n                                          \u0027jobId\u0027: \u0027sched_build_2022-03-21T23-58-05.774\u0027,\n                                          \u0027outcome\u0027: \u0027SUCCESS\u0027,\n                                          \u0027start\u0027: 1647907104839,\n                                          \u0027target\u0027: {\u0027type\u0027: \u0027JOBS\u0027},\n                                          \u0027type\u0027: \u0027JOB_EXECUTED\u0027}],\n               \u0027end\u0027: 1647907142119,\n               \u0027result\u0027: {\u0027end\u0027: 1647907142119,\n                          \u0027outcome\u0027: \u0027SUCCESS\u0027,\n                          \u0027start\u0027: 1647907085692,\n                          \u0027target\u0027: {\u0027projectKey\u0027: \u0027EVERYTHING\u0027,\n                                     \u0027scenarioId\u0027: \u002724811\u0027,\n                                     \u0027stepId\u0027: \u0027build_0_true_d_CoronaSync\u0027,\n                                     \u0027type\u0027: \u0027SCENARIO_STEP\u0027},\n                          \u0027type\u0027: \u0027STEP_DONE\u0027},\n               \u0027retryIndex\u0027: 0,\n               \u0027runId\u0027: \u00272022-03-22-08-58-05-688\u0027,\n               \u0027scenarioRun\u0027: {\u0027clustersUsed\u0027: [],\n                               \u0027end\u0027: 1647907142297,\n                               \u0027reportersStates\u0027: [],\n                               \u0027result\u0027: {\u0027end\u0027: 1647907142295,\n                                          \u0027login\u0027: \u0027admin\u0027,\n                                          \u0027outcome\u0027: \u0027SUCCESS\u0027,\n                                          \u0027start\u0027: 1647907085679,\n                                          \u0027target\u0027: {\u0027projectKey\u0027: \u0027EVERYTHING\u0027,\n                                                     \u0027scenarioId\u0027: \u002724811\u0027,\n                                                     \u0027type\u0027: \u0027SCENARIO\u0027},\n                                          \u0027type\u0027: \u0027SCENARIO_DONE\u0027},\n                               \u0027runId\u0027: \u00272022-03-22-08-58-05-103\u0027,\n                               \u0027scenario\u0027: {\u0027active\u0027: False,\n                                            \u0027automationLocal\u0027: False,\n                                            \u0027checklists\u0027: {\u0027checklists\u0027: []},\n                                            \u0027customFields\u0027: {},\n                                            \u0027delayedTriggersBehavior\u0027: {\u0027delayWhileRunning\u0027: True,\n                                                                        \u0027squashDelayedTriggers\u0027: True,\n                                                                        \u0027suppressTriggersWhileRunning\u0027: True},\n                                            \u0027id\u0027: \u002724811\u0027,\n                                            \u0027name\u0027: \u002724811\u0027,\n                                            \u0027params\u0027: {\u0027steps\u0027: [{\u0027delayBetweenRetries\u0027: 10,\n                                                                  \u0027id\u0027: \u0027build_0_true_d_CoronaSync\u0027,\n                                                                  \u0027maxRetriesOnFail\u0027: 0,\n                                                                  \u0027name\u0027: \u0027Step \u0027\n                                                                          \u0027#1\u0027,\n                                                                  \u0027params\u0027: {\u0027builds\u0027: [{\u0027itemId\u0027: \u0027CoronaSync\u0027,\n                                                                                         \u0027partitionsSpec\u0027: \u0027\u0027,\n                                                                                         \u0027type\u0027: \u0027DATASET\u0027}],\n                                                                             \u0027jobType\u0027: \u0027RECURSIVE_BUILD\u0027,\n                                                                             \u0027proceedOnFailure\u0027: False,\n                                                                             \u0027refreshHiveMetastore\u0027: True},\n                                                                  \u0027resetScenarioStatus\u0027: False,\n                                                                  \u0027runConditionExpression\u0027: \u0027\u0027,\n                                                                  \u0027runConditionStatuses\u0027: [\u0027SUCCESS\u0027,\n                                                                                           \u0027WARNING\u0027],\n                                                                  \u0027runConditionType\u0027: \u0027RUN_IF_STATUS_MATCH\u0027,\n                                                                  \u0027type\u0027: \u0027build_flowitem\u0027},\n                                                                 {\u0027delayBetweenRetries\u0027: 10,\n                                                                  \u0027id\u0027: \u0027custom_python\u0027,\n                                                                  \u0027maxRetriesOnFail\u0027: 0,\n                                                                  \u0027name\u0027: \u0027${NOW}\u0027,\n                                                                  \u0027params\u0027: {\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027INHERIT\u0027},\n                                                                             \u0027proceedOnFailure\u0027: False},\n                                                                  \u0027resetScenarioStatus\u0027: False,\n                                                                  \u0027runConditionExpression\u0027: \u0027\u0027,\n                                                                  \u0027runConditionStatuses\u0027: [\u0027SUCCESS\u0027,\n                                                                                           \u0027WARNING\u0027],\n                                                                  \u0027runConditionType\u0027: \u0027RUN_IF_STATUS_MATCH\u0027,\n                                                                  \u0027type\u0027: \u0027custom_python\u0027}]},\n                                            \u0027projectKey\u0027: \u0027EVERYTHING\u0027,\n                                            \u0027reporters\u0027: [],\n                                            \u0027tags\u0027: [],\n                                            \u0027triggers\u0027: [],\n                                            \u0027type\u0027: \u0027step_based\u0027,\n                                            \u0027versionTag\u0027: {\u0027lastModifiedBy\u0027: {\u0027login\u0027: \u0027admin\u0027},\n                                                           \u0027lastModifiedOn\u0027: 1629723482178,\n                                                           \u0027versionNumber\u0027: 9}},\n                               \u0027start\u0027: 1647907085101,\n                               \u0027trigger\u0027: {\u0027cancelled\u0027: False,\n                                           \u0027params\u0027: {},\n                                           \u0027projectKey\u0027: \u0027EVERYTHING\u0027,\n                                           \u0027runId\u0027: \u00272022-03-22-08-58-05-099\u0027,\n                                           \u0027scenarioId\u0027: \u002724811\u0027,\n                                           \u0027timestamp\u0027: 1647907085097,\n                                           \u0027trigger\u0027: {\u0027active\u0027: False,\n                                                       \u0027delay\u0027: 0,\n                                                       \u0027id\u0027: \u0027manual\u0027,\n                                                       \u0027name\u0027: \u0027API run\u0027,\n                                                       \u0027type\u0027: \u0027manual\u0027}},\n                               \u0027variables\u0027: {}},\n               \u0027start\u0027: 1647907085685,\n               \u0027step\u0027: {\u0027delayBetweenRetries\u0027: 10,\n                        \u0027id\u0027: \u0027build_0_true_d_CoronaSync\u0027,\n                        \u0027maxRetriesOnFail\u0027: 0,\n                        \u0027name\u0027: \u0027Step #1\u0027,\n                        \u0027params\u0027: {\u0027builds\u0027: [{\u0027itemId\u0027: \u0027CoronaSync\u0027,\n                                               \u0027partitionsSpec\u0027: \u0027\u0027,\n                                               \u0027type\u0027: \u0027DATASET\u0027}],\n                                   \u0027jobType\u0027: \u0027RECURSIVE_BUILD\u0027,\n                                   \u0027proceedOnFailure\u0027: False,\n                                   \u0027refreshHiveMetastore\u0027: True},\n                        \u0027resetScenarioStatus\u0027: False,\n                        \u0027runConditionExpression\u0027: \u0027\u0027,\n                        \u0027runConditionStatuses\u0027: [\u0027SUCCESS\u0027, \u0027WARNING\u0027],\n                        \u0027runConditionType\u0027: \u0027RUN_IF_STATUS_MATCH\u0027,\n                        \u0027type\u0027: \u0027build_flowitem\u0027}},\n              {\u0027additionalReportItems\u0027: [],\n               \u0027end\u0027: 1647907142291,\n               \u0027result\u0027: {\u0027end\u0027: 1647907142291,\n                          \u0027outcome\u0027: \u0027SUCCESS\u0027,\n                          \u0027start\u0027: 1647907142129,\n                          \u0027target\u0027: {\u0027projectKey\u0027: \u0027EVERYTHING\u0027,\n                                     \u0027scenarioId\u0027: \u002724811\u0027,\n                                     \u0027stepId\u0027: \u0027custom_python\u0027,\n                                     \u0027type\u0027: \u0027SCENARIO_STEP\u0027},\n                          \u0027type\u0027: \u0027STEP_DONE\u0027},\n               \u0027retryIndex\u0027: 0,\n               \u0027runId\u0027: \u00272022-03-22-08-59-02-126\u0027,\n               \u0027scenarioRun\u0027: {\u0027clustersUsed\u0027: [],\n                               \u0027end\u0027: 1647907142297,\n                               \u0027reportersStates\u0027: [],\n                               \u0027result\u0027: {\u0027end\u0027: 1647907142295,\n                                          \u0027login\u0027: \u0027admin\u0027,\n                                          \u0027outcome\u0027: \u0027SUCCESS\u0027,\n                                          \u0027start\u0027: 1647907085679,\n                                          \u0027target\u0027: {\u0027projectKey\u0027: \u0027EVERYTHING\u0027,\n                                                     \u0027scenarioId\u0027: \u002724811\u0027,\n                                                     \u0027type\u0027: \u0027SCENARIO\u0027},\n                                          \u0027type\u0027: \u0027SCENARIO_DONE\u0027},\n                               \u0027runId\u0027: \u00272022-03-22-08-58-05-103\u0027,\n                               \u0027scenario\u0027: {\u0027active\u0027: False,\n                                            \u0027automationLocal\u0027: False,\n                                            \u0027checklists\u0027: {\u0027checklists\u0027: []},\n                                            \u0027customFields\u0027: {},\n                                            \u0027delayedTriggersBehavior\u0027: {\u0027delayWhileRunning\u0027: True,\n                                                                        \u0027squashDelayedTriggers\u0027: True,\n                                                                        \u0027suppressTriggersWhileRunning\u0027: True},\n                                            \u0027id\u0027: \u002724811\u0027,\n                                            \u0027name\u0027: \u002724811\u0027,\n                                            \u0027params\u0027: {\u0027steps\u0027: [{\u0027delayBetweenRetries\u0027: 10,\n                                                                  \u0027id\u0027: \u0027build_0_true_d_CoronaSync\u0027,\n                                                                  \u0027maxRetriesOnFail\u0027: 0,\n                                                                  \u0027name\u0027: \u0027Step \u0027\n                                                                          \u0027#1\u0027,\n                                                                  \u0027params\u0027: {\u0027builds\u0027: [{\u0027itemId\u0027: \u0027CoronaSync\u0027,\n                                                                                         \u0027partitionsSpec\u0027: \u0027\u0027,\n                                                                                         \u0027type\u0027: \u0027DATASET\u0027}],\n                                                                             \u0027jobType\u0027: \u0027RECURSIVE_BUILD\u0027,\n                                                                             \u0027proceedOnFailure\u0027: False,\n                                                                             \u0027refreshHiveMetastore\u0027: True},\n                                                                  \u0027resetScenarioStatus\u0027: False,\n                                                                  \u0027runConditionExpression\u0027: \u0027\u0027,\n                                                                  \u0027runConditionStatuses\u0027: [\u0027SUCCESS\u0027,\n                                                                                           \u0027WARNING\u0027],\n                                                                  \u0027runConditionType\u0027: \u0027RUN_IF_STATUS_MATCH\u0027,\n                                                                  \u0027type\u0027: \u0027build_flowitem\u0027},\n                                                                 {\u0027delayBetweenRetries\u0027: 10,\n                                                                  \u0027id\u0027: \u0027custom_python\u0027,\n                                                                  \u0027maxRetriesOnFail\u0027: 0,\n                                                                  \u0027name\u0027: \u0027${NOW}\u0027,\n                                                                  \u0027params\u0027: {\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027INHERIT\u0027},\n                                                                             \u0027proceedOnFailure\u0027: False},\n                                                                  \u0027resetScenarioStatus\u0027: False,\n                                                                  \u0027runConditionExpression\u0027: \u0027\u0027,\n                                                                  \u0027runConditionStatuses\u0027: [\u0027SUCCESS\u0027,\n                                                                                           \u0027WARNING\u0027],\n                                                                  \u0027runConditionType\u0027: \u0027RUN_IF_STATUS_MATCH\u0027,\n                                                                  \u0027type\u0027: \u0027custom_python\u0027}]},\n                                            \u0027projectKey\u0027: \u0027EVERYTHING\u0027,\n                                            \u0027reporters\u0027: [],\n                                            \u0027tags\u0027: [],\n                                            \u0027triggers\u0027: [],\n                                            \u0027type\u0027: \u0027step_based\u0027,\n                                            \u0027versionTag\u0027: {\u0027lastModifiedBy\u0027: {\u0027login\u0027: \u0027admin\u0027},\n                                                           \u0027lastModifiedOn\u0027: 1629723482178,\n                                                           \u0027versionNumber\u0027: 9}},\n                               \u0027start\u0027: 1647907085101,\n                               \u0027trigger\u0027: {\u0027cancelled\u0027: False,\n                                           \u0027params\u0027: {},\n                                           \u0027projectKey\u0027: \u0027EVERYTHING\u0027,\n                                           \u0027runId\u0027: \u00272022-03-22-08-58-05-099\u0027,\n                                           \u0027scenarioId\u0027: \u002724811\u0027,\n                                           \u0027timestamp\u0027: 1647907085097,\n                                           \u0027trigger\u0027: {\u0027active\u0027: False,\n                                                       \u0027delay\u0027: 0,\n                                                       \u0027id\u0027: \u0027manual\u0027,\n                                                       \u0027name\u0027: \u0027API run\u0027,\n                                                       \u0027type\u0027: \u0027manual\u0027}},\n                               \u0027variables\u0027: {}},\n               \u0027start\u0027: 1647907142123,\n               \u0027step\u0027: {\u0027delayBetweenRetries\u0027: 10,\n                        \u0027id\u0027: \u0027custom_python\u0027,\n                        \u0027maxRetriesOnFail\u0027: 0,\n                        \u0027name\u0027: \u0027${NOW}\u0027,\n                        \u0027params\u0027: {\u0027envSelection\u0027: {\u0027envMode\u0027: \u0027INHERIT\u0027},\n                                   \u0027proceedOnFailure\u0027: False},\n                        \u0027resetScenarioStatus\u0027: False,\n                        \u0027runConditionExpression\u0027: \u0027\u0027,\n                        \u0027runConditionStatuses\u0027: [\u0027SUCCESS\u0027, \u0027WARNING\u0027],\n                        \u0027runConditionType\u0027: \u0027RUN_IF_STATUS_MATCH\u0027,\n                        \u0027type\u0027: \u0027custom_python\u0027}}]}\n"
        }
      ]
    },
    {
      "execution_count": 3,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nimport pprint as pp\nproject_key\u003d\"EVERYTHING\"\nclient\u003ddataiku.api_client()\n\nplugin \u003d client.get_plugin(\"model-lightgbm\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/Users/rendo/Library/DataScienceStudio/dss_home/code-envs/python/30688/lib/python3.6/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n  warnings.warn(msg)\n"
        }
      ]
    },
    {
      "execution_count": 4,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pp.pprint(plugin)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u003cdataikuapi.dss.plugin.DSSPlugin object at 0x10be19390\u003e\n"
        }
      ]
    },
    {
      "execution_count": 16,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "response \u003d plugin.get_file(\u0027/python-prediction-algos/model-lightgbm_classification/algo.json\u0027)"
      ],
      "outputs": []
    },
    {
      "execution_count": 17,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pp.pprint(response.data)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(b\u0027{\\n    \"meta\" : {\\n        \"label\": \"LightGBM Classification\",\\n        \"de\u0027\n b\u0027scription\": \"LightGBM is a gradient boosting framework that uses tree based \u0027\n b\u0027learning algorithms. It is designed to be distributed and efficient. This cu\u0027\n b\u0027stom algorithm needs to be used with an appropriate code-env.\",\\n        \u0027\n b\u0027\"icon\": \"icon-list-ul\"\\n    },\\n    \"predictionTypes\": [\"BINARY_CLASSIFICA\u0027\n b\u0027TION\", \"MULTICLASS\"],\\n    \"gridSearchMode\": \"MANAGED\",\\n    \"supportsSamp\u0027\n b\u0027leWeights\": true,\\n    \"acceptsSparseMatrix\": false,\\n    \"params\": [\\n    \u0027\n b\u0027    {\\n            \"name\": \"boosting_type\",\\n            \"label\": \"Boostin\u0027\n b\u0027g type\",\\n            \"description\": \"\",\\n            \"type\": \"MULTISELECT\u0027\n b\u0027\",\\n            \"defaultValue\": [\"gbdt\"],\\n            \"selectChoices\": [\\n\u0027\n b\u0027                {\\n                    \"value\":\"gbdt\",\\n                  \u0027\n b\u0027  \"label\":\"Traditional Gradient Boosting Decision Tree\"\\n                \u0027\n b\u0027},\\n                {\\n                    \"value\":\"dart\",\\n               \u0027\n b\u0027     \"label\":\"Dropouts meet Multiple Additive Regression Trees\"\\n        \u0027\n b\u0027        },\\n                {\\n                    \"value\":\"goss\",\\n       \u0027\n b\u0027             \"label\": \"Gradient-based One-Side Sampling\"\\n               \u0027\n b\u0027 },\\n                {\\n                    \"value\":\"rf\",\\n                \u0027\n b\u0027    \"label\": \"Random Forest\"\\n                }\\n            ],\\n          \u0027\n b\u0027  \"gridParam\": true\\n        },\\n        {\\n            \"name\": \"num_leaves\u0027\n b\u0027\",\\n            \"label\": \"Num leaves\",\\n            \"description\": \"Maximu\u0027\n b\u0027m tree leaves for base learners.\",\\n            \"type\": \"DOUBLES\",\\n      \u0027\n b\u0027      \"defaultValue\": [50, 100],\\n            \"allowDuplicates\": false,\\n \u0027\n b\u0027           \"gridParam\": true\\n        },\\n        {\\n            \"name\": \"m\u0027\n b\u0027ax_depth\",\\n            \"label\": \"Max depth\",\\n            \"description\": \u0027\n b\u0027\"Maximum tree depth for base learners, \u003c\u003d0 means no limit\",\\n            \u0027\n b\u0027\"type\": \"DOUBLES\",\\n            \"defaultValue\": [50, 100],\\n            \"a\u0027\n b\u0027llowDuplicates\": false,\\n            \"gridParam\": true\\n        },\\n       \u0027\n b\u0027 {\\n            \"name\": \"learning_rate\",\\n            \"label\": \"Learning r\u0027\n b\u0027ate\",\\n            \"description\":\"Lower values slow down convergence and \u0027\n b\u0027can make the model more robust. Typical values: 0.01 - 0.3\",\\n           \u0027\n b\u0027 \"type\": \"DOUBLES\",\\n            \"defaultValue\": [0.2],\\n            \"allo\u0027\n b\u0027wDuplicates\": false,\\n            \"gridParam\": true\\n        },\\n        {\\n\u0027\n b\u0027            \"name\": \"n_estimators\",\\n            \"label\": \"Num estimators\u0027\n b\u0027\",\\n            \"description\": \"The maximum number of estimators at which\u0027\n b\u0027 boosting is terminated. In case of perfect fit, the learning procedure is s\u0027\n b\u0027topped early.\",\\n            \"type\": \"DOUBLES\",\\n            \"defaultValue\u0027\n b\u0027\": [50, 100],\\n            \"allowDuplicates\": false,\\n            \"gridPar\u0027\n b\u0027am\": true\\n        },\\n        {\\n            \"name\": \"early_stopping\",\\n   \u0027\n b\u0027         \"label\": \"Early stopping\",\\n            \"description\": \" Use Lig\u0027\n b\u0027htGBM built-in early stop mechanism so the exact number of trees will be opt\u0027\n b\u0027imized.\",\\n            \"type\": \"BOOLEAN\"\\n        },\\n        {\\n           \u0027\n b\u0027 \"name\": \"early_stopping_rounds\",\\n            \"label\": \"Early stopping r\u0027\n b\u0027ounds\",\\n            \"description\": \"The optimizer stops if the loss neve\u0027\n b\u0027r decreases for this consecutive number of iterations. Typical values: 1 - 1\u0027\n b\u002700\",\\n            \"type\": \"INT\",\\n            \"defaultValue\": 4,\\n         \u0027\n b\u0027   \"visibilityCondition\": \"model.early_stopping\"\\n        },\\n        {\\n  \u0027\n b\u0027          \"name\": \"min_split_gain\",\\n            \"label\": \"Min split gain\u0027\n b\u0027\",\\n            \"description\":\"Minimum loss reduction required to make a \u0027\n b\u0027further partition on a leaf node of the tree.\",\\n            \"type\": \"DOU\u0027\n b\u0027BLES\",\\n            \"defaultValue\": [0],\\n            \"allowDuplicates\": f\u0027\n b\u0027alse,\\n            \"gridParam\": true\\n        },\\n        {\\n            \"na\u0027\n b\u0027me\": \"min_child_weight\",\\n            \"label\": \"Min child weight\",\\n      \u0027\n b\u0027      \"description\":\"Minimum sum of instance weight (hessian) needed in a ch\u0027\n b\u0027ild (leaf).\",\\n            \"type\": \"DOUBLES\",\\n            \"defaultValue\":\u0027\n b\u0027 [0.001],\\n            \"allowDuplicates\": false,\\n            \"gridParam\":\u0027\n b\u0027 true\\n        },\\n        {\\n            \"name\": \"min_child_samples\",\\n    \u0027\n b\u0027        \"label\": \"Min child samples\",\\n            \"description\":\"Minimum\u0027\n b\u0027 number of data needed in a child (leaf).\",\\n            \"type\": \"DOUBLES\u0027\n b\u0027\",\\n            \"defaultValue\": [20],\\n            \"allowDuplicates\": fals\u0027\n b\u0027e,\\n            \"gridParam\": true\\n        },\\n        {\\n            \"name\"\u0027\n b\u0027: \"subsample\",\\n            \"label\": \"Subsample\",\\n            \"descriptio\u0027\n b\u0027n\":\"Subsample ratio of the training instance.\",\\n            \"type\": \"DOU\u0027\n b\u0027BLES\",\\n            \"defaultValue\": [1.0],\\n            \"allowDuplicates\":\u0027\n b\u0027 false,\\n            \"gridParam\": true\\n        },\\n        {\\n            \"\u0027\n b\u0027name\": \"subsample_freq\",\\n            \"label\": \"Subsample freq\",\\n        \u0027\n b\u0027    \"description\":\"Frequence of subsample, \u003c\u003d0 means no enable.\",\\n      \u0027\n b\u0027      \"type\": \"DOUBLES\",\\n            \"defaultValue\": [0],\\n            \"a\u0027\n b\u0027llowDuplicates\": false,\\n            \"gridParam\": true\\n        },\\n       \u0027\n b\u0027 {\\n            \"name\": \"colsample_bytree\",\\n            \"label\": \"Col sam\u0027\n b\u0027ple by tree\",\\n            \"description\":\"Subsample ratio of columns when\u0027\n b\u0027 constructing each tree.\",\\n            \"type\": \"DOUBLES\",\\n            \"d\u0027\n b\u0027efaultValue\": [1.0],\\n            \"allowDuplicates\": false,\\n            \"\u0027\n b\u0027gridParam\": true\\n        },\\n        {\\n            \"name\": \"reg_alpha\",\\n \u0027\n b\u0027           \"label\": \"Reg alpha\",\\n            \"description\":\"L1 regulariz\u0027\n b\u0027ation term on weights.\",\\n            \"type\": \"DOUBLES\",\\n            \"def\u0027\n b\u0027aultValue\": [0.0],\\n            \"allowDuplicates\": false,\\n            \"gr\u0027\n b\u0027idParam\": true\\n        },\\n        {\\n            \"name\": \"reg_lambda\",\\n  \u0027\n b\u0027          \"label\": \"Reg lambda\",\\n            \"description\":\"L2 regulariz\u0027\n b\u0027ation term on weights.\",\\n            \"type\": \"DOUBLES\",\\n            \"def\u0027\n b\u0027aultValue\": [0.0],\\n            \"allowDuplicates\": false,\\n            \"gr\u0027\n b\u0027idParam\": true\\n        },\\n        {\\n            \"name\": \"random_state\",\\n\u0027\n b\u0027            \"label\": \"Random state\",\\n            \"description\": \"Using a\u0027\n b\u0027 fixed random seed allows for reproducible result.\",\\n            \"type\":\u0027\n b\u0027 \"DOUBLE\",\\n            \"defaultValue\": 300\\n        },\\n        {\\n        \u0027\n b\u0027    \"name\": \"n_jobs\",\\n            \"label\": \"Parallelism\",\\n            \"d\u0027\n b\u0027escription\": \"Number of cores used for parallel training. Using more cores l\u0027\n b\u0027eads to faster training but at the expense of more memory consumption, espec\u0027\n b\u0027ially for large training datasets. (-1 means \\\u0027all cores\\\u0027)\",\\n            \u0027\n b\u0027\"type\": \"DOUBLE\",\\n            \"defaultValue\": 4\\n        }\\n    ]\\n}\\n\u0027)\n"
        }
      ]
    },
    {
      "execution_count": 7,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "project \u003d client.get_project(\"EVERYTHING\")\nnew_mltask \u003d project.create_prediction_ml_task(\ninput_dataset\u003d\"2_prepared\", target_variable\u003d\"col_0\", guess_policy\u003d\u0027DEFAULT\u0027)"
      ],
      "outputs": []
    },
    {
      "execution_count": 12,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "test \u003d new_mltask.get_settings().get_raw()\ntest[\u0027modeling\u0027][\u0027plugin_python\u0027][\u0027CustomPyPredAlgo_model-lightgbm_model-lightgbm_classification\u0027][\u0027params\u0027][\u0027random_state\u0027]\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "metadata": {},
          "data": {
            "text/plain": "10.0"
          },
          "execution_count": 12
        }
      ]
    },
    {
      "execution_count": 26,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "new_mltask.get_settings().get_raw()[\u0027modeling\u0027][\u0027plugin_python\u0027][\u0027CustomPyPredAlgo_model-lightgbm_model-lightgbm_classification\u0027][\u0027params\u0027][\u0027random_state\u0027]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "metadata": {},
          "data": {
            "text/plain": "1337.0"
          },
          "execution_count": 26
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}