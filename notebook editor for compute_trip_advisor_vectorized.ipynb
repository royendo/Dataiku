{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "tags": [
      "recipe-editor"
    ],
    "associatedRecipe": "compute_trip_advisor_vectorized",
    "createdOn": 1622806297805,
    "hide_input": false,
    "modifiedBy": "admin",
    "customFields": {},
    "creator": "admin"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 1,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\nfrom gensim import corpora\nfrom gensim import models\nimport MeCab\nfrom gensim.models import word2vec\nfrom gensim.models import TfidfModel\nfrom operator import itemgetter"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/data.ssd/dataiku/dss/code-envs/python/japan-nlp__1_/lib/python3.6/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\nscipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n  _deprecated()\n"
        }
      ]
    },
    {
      "execution_count": 2,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Read recipe inputs\nclean_trip_advisor \u003d dataiku.Dataset(\"clean_trip_advisor\")\ndf \u003d clean_trip_advisor.get_dataframe()\n\nmodel_folder_path \u003d dataiku.Folder(\"m9JZdV7b\").get_path()\nmodel_path \u003d model_folder_path + \"/word2vec_ramen_model.model\"\nramen_model \u003d word2vec.Word2Vec.load(model_path)\n\nwakati_folder \u003d dataiku.Folder(\"4kJbig2u\").get_path()\ntagger_path \u003d \u0027-Owakati -r {} -d {}\u0027.format(wakati_folder, wakati_folder)"
      ],
      "outputs": []
    },
    {
      "execution_count": 4,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tagger \u003d MeCab.Tagger(tagger_path)#タグはMeCab.Tagger（neologd辞書）を使用\n#tagger.parse(\u0027\u0027)"
      ],
      "outputs": []
    },
    {
      "execution_count": 5,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def tokenize_ja(text, lower):\n    node \u003d tagger.parseToNode(str(text))\n    while node:\n        if lower and node.feature.split(\u0027,\u0027)[0] in [\"名詞\",\"形容詞\"]:#分かち書きで取得する品詞を指定\n            yield node.surface.lower()\n        node \u003d node.next\ndef tokenize(content, token_min_len, token_max_len, lower):\n    return [\n        str(token) for token in tokenize_ja(content, lower)\n        if token_min_len \u003c\u003d len(token) \u003c\u003d token_max_len and not token.startswith(\u0027_\u0027)\n    ]"
      ],
      "outputs": []
    },
    {
      "execution_count": 6,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#コーパス作成\nwakati_ramen_text \u003d []\nfor i in df[\u0027review\u0027]:\n    txt \u003d tokenize(i, 2, 10000, True)\n    wakati_ramen_text.append(txt)"
      ],
      "outputs": []
    },
    {
      "execution_count": 7,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vocab \u003d list(set([item for sublist in wakati_ramen_text for item in sublist]))"
      ],
      "outputs": []
    },
    {
      "execution_count": 8,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "len(vocab)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "metadata": {},
          "data": {
            "text/plain": "205"
          },
          "execution_count": 8
        }
      ]
    },
    {
      "execution_count": 9,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# max_vocab \u003d 30000 #40000にしても結果は同じだった\n# vocab \u003d list(word2vec_ramen_model.wv.vocab.keys())[:max_vocab]\n# vectors \u003d [ramen_model.wv[word] for word in vocab]\nvectors \u003d []\nvocabs \u003d []\nfor word in vocab:\n    try:\n        vector \u003d ramen_model.wv[word]\n        vectors.append(vector)\n        vocabs.append(word)\n    except:\n        None"
      ],
      "outputs": []
    },
    {
      "execution_count": 10,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#len(vectors)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "metadata": {},
          "data": {
            "text/plain": "173"
          },
          "execution_count": 10
        }
      ]
    },
    {
      "execution_count": 11,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vocab_df \u003d pd.DataFrame(vectors)\nvocab_df[\u0027words\u0027] \u003d vocabs"
      ],
      "outputs": []
    },
    {
      "execution_count": 12,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "trip_advisor_vectorized \u003d dataiku.Dataset(\"trip_advisor_vectorized\")\ntrip_advisor_vectorized.write_with_schema(vocab_df)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "173 rows successfully written (rHOj01fTaZ)\n"
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}